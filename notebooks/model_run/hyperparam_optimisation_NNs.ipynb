{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "#!pip install torch_sparse"
   ],
   "metadata": {
    "id": "MGbECsd4zgcH"
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ],
   "metadata": {
    "id": "6OpO9CfLIFQt",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5c1c2e2e-2b24-4fae-894c-dcf5bbf76f97"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "import sklearn.linear_model\n",
    "\n",
    "\"\"\"Define graph forming functions\"\"\"\n",
    "# Import packages\n",
    "\n",
    "import math\n",
    "import joblib\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "#from google.colab import files\n",
    "\n",
    "from sklearn import tree\n",
    "import sklearn.svm as svm\n",
    "from sklearn import ensemble\n",
    "import sklearn.metrics as skm\n",
    "import sklearn.preprocessing as pp\n",
    "import sklearn.linear_model as lms\n",
    "import sklearn.neural_network as skl_nn\n",
    "import sklearn.neighbors as neighbors\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import sklearn.model_selection as model_sel\n",
    "\n",
    "import torch\n",
    "\n",
    "#import torch_sparse\n",
    "import torch.nn as nn\n",
    "# from torch_geometric.data import Data\n",
    "# from torch_geometric.nn import GCNConv"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "oEBsNP4nzFhS"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Input data\n",
    "random_state = 2\n",
    "np.random.seed(random_state)\n",
    "data_path = \"/content/drive/MyDrive/PhD - Climate Change Mitigation/data/GRL_for_IE/\"\n",
    "#'C:/Users\\lukec\\PycharmProjects\\emissions-tracking-conda\\emissions-tracking\\models\\datasets/'\n",
    "\n",
    "max_test_set = 100000"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "NFUHGM_xzFhV"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def filter_for_start_yr(df, start_col, end_col) -> pd.DataFrame:\n",
    "    \"\"\"Convert dataframe of plants with entry for each year into dataframe with row for each year\"\"\"\n",
    "    # Get rid of emissions for years before start year\n",
    "    df['Age'] = df['Year'].astype(int) - df[start_col].astype(int)\n",
    "    df = df[df['Age'] >= 0]\n",
    "\n",
    "    # Get rid of emissions for years after end years\n",
    "    df['ToGo'] = df[end_col].astype(int) - df['Year'].astype(int)\n",
    "    df = df[df['ToGo'] >= 0]\n",
    "\n",
    "    df[['START_YR', 'END_YR']] = df[['START_YR', 'END_YR']].astype(int)\n",
    "\n",
    "    return df.drop(columns=['ToGo'])\n",
    "\n",
    "def pivot_data_dfs(df:pd.DataFrame, time_col) -> [pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Pivot data dataframes to get each entry and all year/month values in rows\"\"\"\n",
    "    feature_cols = [i for i in df.columns if i not in [time_col, 'Emissions']]\n",
    "    df_pivoted = df.pivot(index=feature_cols,\n",
    "                          columns=time_col,\n",
    "                          values = 'Emissions').reset_index()\n",
    "    return df_pivoted, feature_cols\n",
    "\n",
    "\n",
    "def melt_data_dfs(df:pd.DataFrame, feature_cols, time_col) -> pd.DataFrame:\n",
    "    \"\"\"Melt data dataframes to get row per date entry for each facility\"\"\"\n",
    "    melted = df.melt(id_vars=feature_cols, var_name=time_col, value_name='Emissions').dropna(subset=['Emissions'])\n",
    "    melted[time_col] = melted[time_col].astype(int)\n",
    "    \n",
    "    if 'START_YR' in df.columns and 'END_YR' in df.columns:\n",
    "        melted = filter_for_start_yr(melted, 'START_YR', 'END_YR')\n",
    "\n",
    "    return melted\n",
    "\n",
    "## Convert train and test sets into ML ready sets\n",
    "def series_to_bins(series:pd.Series, bins:list=None, labels:list=None, positive:bool=True):\n",
    "    # Convert a continuous pandas dataframe column into discrete bins\n",
    "    if bins is None:\n",
    "        bin_series = series[series!=0] if positive else series\n",
    "        bins = [min(bin_series.min(),0)-0.01, bin_series.quantile(0.25), bin_series.quantile(0.5), bin_series.quantile(0.75), bin_series.max()+0.01]\n",
    "    if labels is None: labels = list(range(len(bins)-1))\n",
    "\n",
    "    transformer = pp.FunctionTransformer(\n",
    "        pd.cut, kw_args={'bins': bins, 'labels': labels, 'retbins': False}\n",
    "    )\n",
    "    return bins, transformer.fit_transform(series)\n",
    "\n",
    "\n",
    "def preprocess_yearly(train_set, test_set, y_col='Emissions'):\n",
    "    \"\"\"Digitise train and test sets\"\"\"\n",
    "\n",
    "    # Create Y\n",
    "    bins, y_train_clf = series_to_bins(train_set[y_col])\n",
    "    _, y_test_clf = series_to_bins(test_set[y_col], bins=[test_set[y_col].min()-0.01]+bins[1:-1]+[test_set[y_col].max()+0.01])\n",
    "\n",
    "    y_train_reg, y_test_reg = train_set[y_col], test_set[y_col]\n",
    "    train_set, test_set = train_set.drop(columns=[y_col]), test_set.drop(columns=[y_col])\n",
    "\n",
    "    # Create X\n",
    "    # Deal with string columns\n",
    "    x_enc = pp.OrdinalEncoder()\n",
    "    string_cols = list(train_set.select_dtypes(include='object').columns)\n",
    "    train_set[string_cols] = train_set[string_cols].astype(str)\n",
    "    test_set[string_cols] = test_set[string_cols].astype(str)\n",
    "    x_strings = pd.concat((train_set[string_cols], test_set[string_cols]))\n",
    "    x_enc.fit(x_strings)\n",
    "\n",
    "    # Make float columns into int columns\n",
    "    float_cols = list(train_set.select_dtypes(include='float').columns)\n",
    "    train_set[float_cols], test_set[float_cols] = train_set[float_cols].astype(int), test_set[float_cols].astype(int)\n",
    "\n",
    "    if 'LATITUDE' in list(train_set.columns) and 'LONGITUDE' in list(train_set.columns):\n",
    "        train_set[['LATITUDE', 'LONGITUDE']] = (train_set[['LATITUDE', 'LONGITUDE']].astype(int)+[90, 180])\n",
    "        test_set[['LATITUDE', 'LONGITUDE']] = (test_set[['LATITUDE', 'LONGITUDE']].astype(int)+[90, 180])\n",
    "\n",
    "    int_cols = list(train_set.select_dtypes(include='integer').columns)\n",
    "    x_ints_min = pd.concat((train_set[int_cols], test_set[int_cols])).min().values\n",
    "    x_ints_train = train_set[int_cols] - x_ints_min\n",
    "    x_ints_test = test_set[int_cols] - x_ints_min\n",
    "\n",
    "    X_train = np.concatenate((x_enc.transform(train_set[string_cols]),\n",
    "                              x_ints_train.values), axis=1)\n",
    "    X_test = np.concatenate((x_enc.transform(test_set[string_cols]),\n",
    "                              x_ints_test.values), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    return X_train, X_test, y_train_clf, y_test_clf, y_train_reg, y_test_reg, x_enc\n",
    "\n",
    "def save_decoded_X(X, x_enc, cols, used, name):\n",
    "    min_years = [used['START_YR'].astype(int).min(), 1978]\n",
    "    X_inv = np.concatenate((x_enc.inverse_transform(X[:,:-4]), (X[:,-4:-2]+min_years).astype(int), X[:,-2:]), axis=1)\n",
    "    pd.DataFrame(X_inv, columns=list(columns[:-2]+['Year']+columns[-2:])).to_csv(name+'.csv')\n",
    "\n",
    "# Function to split rows into two DataFrames\n",
    "def split_rows(group, test_fraction):\n",
    "\n",
    "    num_rows = group.shape[0]\n",
    "    if num_rows == 1:\n",
    "        return None, None  # Exclude groups with only one sample\n",
    "\n",
    "    num_sampled_rows = int(min(max(test_fraction * num_rows, 1), num_rows-1))  # At least one sample for each group\n",
    "\n",
    "    test_df = group.sample(n=num_sampled_rows)\n",
    "    train_df = group.drop(test_df.index)\n",
    "\n",
    "    return train_df, test_df"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "L0-1GPoPzFhW"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def custom_interpolate(row):\n",
    "    if row.count() >= 3:  # Check if there are enough values for polynomial interpolation\n",
    "        return row.interpolate(method='polynomial', order=order, limit_direction='both')\n",
    "    else:\n",
    "        return row.interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "def metrics(y_true, y_pred, model_type='clf'):\n",
    "    if model_type == 'clf':\n",
    "        metric_dict = {'confusion': skm.confusion_matrix(y_true, y_pred),\n",
    "                       'overall_acc': skm.accuracy_score(y_true, y_pred),\n",
    "                       'average_acc': skm.balanced_accuracy_score(y_true, y_pred),\n",
    "                       'kappa': skm.cohen_kappa_score(y_true, y_pred),\n",
    "                       'IoU': skm.jaccard_score(y_true, y_pred, average='weighted')}\n",
    "    elif model_type == 'reg':\n",
    "        metric_dict = {'r2': skm.r2_score(y_true, y_pred),\n",
    "                       'mae': skm.mean_absolute_error(y_true, y_pred),\n",
    "                       'mse': skm.mean_squared_error(y_true, y_pred)}\n",
    "\n",
    "    else: raise 'Incorrect model type'\n",
    "\n",
    "    return metric_dict"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "u4mIUn4vzFhY"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "## Define torch models and functions\n",
    "\n",
    "class DeepNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim):\n",
    "        super(DeepNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_dim2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "# class ConvNet(nn.Module):\n",
    "#     def __init__(self, input_channels, output_dim):\n",
    "#         super(ConvNet, self).__init__()\n",
    "#         self.conv1 = nn.Conv1d(input_channels, 16, kernel_size=2)\n",
    "#         self.pool1 = nn.MaxPool1d(kernel_size=1)\n",
    "#         self.conv2 = nn.Conv1d(16, 32, kernel_size=2)\n",
    "#         self.pool2 = nn.MaxPool1d(kernel_size=1)\n",
    "#         self.fc1 = nn.Linear(32 * 2, 256)\n",
    "#         self.relu1 = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(256, output_dim)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         batch_size = x.size(0)\n",
    "#         out = self.conv1(x)\n",
    "#         out = self.pool1(out)\n",
    "#         out = self.conv2(out)\n",
    "#         #out = self.pool2(out)\n",
    "#         out = out.view(batch_size, -1)\n",
    "#         out = self.fc1(out)\n",
    "#         out = self.relu1(out)\n",
    "#         out = self.fc2(out)\n",
    "#         return out\n",
    "#\n",
    "#     def _calculate_fc_input_size(self):\n",
    "#         # Calculate the input size for the fully connected layer\n",
    "#         dummy_input = torch.zeros(1, 1, 9)\n",
    "#         out = self.conv1(dummy_input)\n",
    "#         out = self.pool1(out)\n",
    "#         # out = self.conv2(out)\n",
    "#         # out = self.pool2(out)\n",
    "#         return out.size(2) * 32\n",
    "\n",
    "\n",
    "def train(model, x_train, y_train, optimizer, criterion=torch.nn.CrossEntropyLoss(), epochs=100, verbose=True):\n",
    "    \"\"\"Training function for pytorch models\"\"\"\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "        y_pred = model(x_train) # Forward pass\n",
    "        loss = criterion(y_pred, y_train) # Compute loss\n",
    "        if verbose:\n",
    "            print('Epoch {}: train loss: {}'.format(epoch, loss.item()))\n",
    "        loss.backward()  # Derive gradients\n",
    "        optimizer.step()  # Update parameters based on gradients\n",
    "\n",
    "    return loss, model, optimizer\n",
    "\n",
    "\n",
    "def test(model, x_test, categorical=True):\n",
    "    \"\"\"Test function for pytorch models\"\"\"\n",
    "    model.eval()\n",
    "    y_pred = model(x_test) # Forward pass\n",
    "\n",
    "    if categorical: # Get category with the highest probability\n",
    "        _, y_pred_cats = torch.max(y_pred, dim = 1)\n",
    "        return y_pred, y_pred_cats\n",
    "    else: # Return raw prediction\n",
    "        return y_pred\n",
    "\n",
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(LSTMNet, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, input_channels, output_dim, num_blocks=4):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channels, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.residual_blocks = self._make_residual_blocks(16, num_blocks)\n",
    "        self.fc = nn.Linear(16, output_dim)\n",
    "\n",
    "    def _make_residual_blocks(self, channels, num_blocks):\n",
    "        layers = []\n",
    "        for i in range(num_blocks):\n",
    "            layers.append(ResidualBlock(channels, channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.residual_blocks(out)\n",
    "        out = torch.mean(out, dim=2)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# class TransformerBlock(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim, num_heads, dropout):\n",
    "#         super(TransformerBlock, self).__init__()\n",
    "#         self.attention = nn.MultiheadAttention(input_dim, num_heads, dropout=dropout)\n",
    "#         self.dropout1 = nn.Dropout(dropout)\n",
    "#         self.norm1 = nn.LayerNorm(input_dim)\n",
    "#         self.fc = nn.Linear(input_dim, hidden_dim)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.dropout2 = nn.Dropout(dropout)\n",
    "#         self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         out, _ = self.attention(x, x, x)\n",
    "#         out = self.dropout1(out)\n",
    "#         out = self.norm1(x + out)\n",
    "#         out = self.fc(out)\n",
    "#         out = self.relu(out)\n",
    "#         out = self.dropout2(out)\n",
    "#         out = self.norm2(out)\n",
    "#         return out\n",
    "#\n",
    "#\n",
    "# class TransformerNet(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim, num_heads, num_blocks, output_dim, dropout):\n",
    "#         super(TransformerNet, self).__init__()\n",
    "#         self.transformer_blocks = self._make_transformer_blocks(input_dim, hidden_dim, num_heads, num_blocks, dropout)\n",
    "#         self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "#\n",
    "#     def _make_transformer_blocks(self, input_dim, hidden_dim, num_heads, num_blocks, dropout):\n",
    "#         layers = []\n",
    "#         for _ in range(num_blocks):\n",
    "#             layers.append(TransformerBlock(input_dim, hidden_dim, num_heads, dropout))\n",
    "#         return nn.Sequential(*layers)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         out = self.transformer_blocks(x)\n",
    "#         out = torch.mean(out, dim=1)\n",
    "#         out = self.fc(out)\n",
    "#         return out\n",
    "\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "class TransformerNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=256, num_layers=4, num_heads=8, dropout=0.1):\n",
    "        super(TransformerNet, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
    "        self.pos_encoding = PositionalEncoding(hidden_dim)\n",
    "        encoder_layer = TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = torch.mean(x, dim=1)  # Average pooling over sequence length\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, hidden_dim, max_seq_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "        pe = torch.zeros(max_seq_len, hidden_dim)\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, hidden_dim, 2).float() * (-math.log(10000.0) / hidden_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=2, num_heads=2, dropout=0.2):\n",
    "        super(TransformerNet, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.unsqueeze(0)  # Add a batch dimension\n",
    "        x = x.permute(1, 0, 2)  # Permute dimensions for transformer input\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.mean(dim=0)  # Average pooling\n",
    "        x = self.fc(x)\n",
    "        return x"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "49Ax3h84zFhd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def balance_classes_pt(X_train, X_test, y_train, y_test, col_name = 'Emissions'):\n",
    "    y_train_pd = pd.Series(y_train, name=col_name)\n",
    "    min_count = y_train_pd.reset_index().groupby(col_name).count().min()\n",
    "    y_train_df = y_train_pd.reset_index().groupby(col_name).sample(min_count.values)\n",
    "    y_train = y_train_df[col_name].values\n",
    "    X_train = X_train[y_train_df.index]\n",
    "\n",
    "    y_test_pd = pd.Series(y_test, name=col_name)\n",
    "    min_count = y_test_pd.reset_index().groupby(col_name).count().min()\n",
    "    y_test_df = y_test_pd.reset_index().groupby(col_name).sample(min_count.values)\n",
    "    y_test = y_test_df[col_name].values\n",
    "    X_test = X_test[y_test_df.index]\n",
    "\n",
    "    return X_train, X_test, torch.tensor(y_train), torch.tensor(y_test)"
   ],
   "metadata": {
    "id": "jzKgS2IqwRPR"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "petrochemicals\n",
      "Gap level 2\n",
      "Training ResNet\n",
      "0.1\n",
      "128\n",
      "Epoch 0: train loss: 1.7057759761810303\n",
      "Epoch 1: train loss: 2.1262927055358887\n",
      "Epoch 2: train loss: 2.043576717376709\n",
      "Epoch 3: train loss: 1.6805980205535889\n",
      "Epoch 4: train loss: 1.5312809944152832\n",
      "Epoch 5: train loss: 1.4587137699127197\n",
      "Epoch 6: train loss: 1.4209082126617432\n",
      "Epoch 7: train loss: 1.4029253721237183\n",
      "Epoch 8: train loss: 1.3970422744750977\n",
      "Epoch 9: train loss: 1.3973758220672607\n",
      "Epoch 10: train loss: 1.400516390800476\n",
      "Epoch 11: train loss: 1.4033150672912598\n",
      "Epoch 12: train loss: 1.4038201570510864\n",
      "Epoch 13: train loss: 1.4009045362472534\n",
      "Epoch 14: train loss: 1.394986867904663\n",
      "Epoch 15: train loss: 1.3881986141204834\n",
      "Epoch 16: train loss: 1.3820364475250244\n",
      "Epoch 17: train loss: 1.380053997039795\n",
      "Epoch 18: train loss: 1.380537509918213\n",
      "Epoch 19: train loss: 1.3770114183425903\n",
      "Epoch 20: train loss: 1.3724151849746704\n",
      "Epoch 21: train loss: 1.3703359365463257\n",
      "Epoch 22: train loss: 1.3703573942184448\n",
      "Epoch 23: train loss: 1.3711246252059937\n",
      "Epoch 24: train loss: 1.3715970516204834\n",
      "Epoch 25: train loss: 1.3711626529693604\n",
      "Epoch 26: train loss: 1.3698081970214844\n",
      "Epoch 27: train loss: 1.3679884672164917\n",
      "Epoch 28: train loss: 1.366040587425232\n",
      "Epoch 29: train loss: 1.3639307022094727\n",
      "Epoch 30: train loss: 1.362031102180481\n",
      "Epoch 31: train loss: 1.360521674156189\n",
      "Epoch 32: train loss: 1.359229564666748\n",
      "Epoch 33: train loss: 1.3580161333084106\n",
      "Epoch 34: train loss: 1.3570556640625\n",
      "Epoch 35: train loss: 1.3557568788528442\n",
      "Epoch 36: train loss: 1.3534550666809082\n",
      "Epoch 37: train loss: 1.3501213788986206\n",
      "Epoch 38: train loss: 1.3462498188018799\n",
      "Epoch 39: train loss: 1.3434895277023315\n",
      "Epoch 40: train loss: 1.3413612842559814\n",
      "Epoch 41: train loss: 1.3373712301254272\n",
      "Epoch 42: train loss: 1.333817958831787\n",
      "Epoch 43: train loss: 1.3305726051330566\n",
      "Epoch 44: train loss: 1.3270756006240845\n",
      "Epoch 45: train loss: 1.3242967128753662\n",
      "Epoch 46: train loss: 1.3190760612487793\n",
      "Epoch 47: train loss: 1.3157284259796143\n",
      "Epoch 48: train loss: 1.3116153478622437\n",
      "Epoch 49: train loss: 1.3076817989349365\n",
      "Epoch 50: train loss: 1.3047850131988525\n",
      "Epoch 51: train loss: 1.2999423742294312\n",
      "Epoch 52: train loss: 1.2935816049575806\n",
      "Epoch 53: train loss: 1.2864892482757568\n",
      "Epoch 54: train loss: 1.2810417413711548\n",
      "Epoch 55: train loss: 1.2805358171463013\n",
      "Epoch 56: train loss: 1.2898690700531006\n",
      "Epoch 57: train loss: 1.2892460823059082\n",
      "Epoch 58: train loss: 1.2638331651687622\n",
      "Epoch 59: train loss: 1.2802971601486206\n",
      "Epoch 60: train loss: 1.2587281465530396\n",
      "Epoch 61: train loss: 1.2664464712142944\n",
      "Epoch 62: train loss: 1.2492578029632568\n",
      "Epoch 63: train loss: 1.2532885074615479\n",
      "Epoch 64: train loss: 1.2416104078292847\n",
      "Epoch 65: train loss: 1.2412455081939697\n",
      "Epoch 66: train loss: 1.2383863925933838\n",
      "Epoch 67: train loss: 1.2277883291244507\n",
      "Epoch 68: train loss: 1.2332967519760132\n",
      "Epoch 69: train loss: 1.2235442399978638\n",
      "Epoch 70: train loss: 1.2150822877883911\n",
      "Epoch 71: train loss: 1.2156299352645874\n",
      "Epoch 72: train loss: 1.213500738143921\n",
      "Epoch 73: train loss: 1.204578161239624\n",
      "Epoch 74: train loss: 1.1961029767990112\n",
      "Epoch 75: train loss: 1.1924338340759277\n",
      "Epoch 76: train loss: 1.1928670406341553\n",
      "Epoch 77: train loss: 1.195220708847046\n",
      "Epoch 78: train loss: 1.1885710954666138\n",
      "Epoch 79: train loss: 1.1735495328903198\n",
      "Epoch 80: train loss: 1.1681559085845947\n",
      "Epoch 81: train loss: 1.1664299964904785\n",
      "Epoch 82: train loss: 1.1590824127197266\n",
      "Epoch 83: train loss: 1.1493598222732544\n",
      "Epoch 84: train loss: 1.1489830017089844\n",
      "Epoch 85: train loss: 1.1494969129562378\n",
      "Epoch 86: train loss: 1.1432944536209106\n",
      "Epoch 87: train loss: 1.1350789070129395\n",
      "Epoch 88: train loss: 1.1296056509017944\n",
      "Epoch 89: train loss: 1.124058485031128\n",
      "Epoch 90: train loss: 1.1305590867996216\n",
      "Epoch 91: train loss: 1.1245940923690796\n",
      "Epoch 92: train loss: 1.1166077852249146\n",
      "Epoch 93: train loss: 1.1160469055175781\n",
      "Epoch 94: train loss: 1.1077467203140259\n",
      "Epoch 95: train loss: 1.104527235031128\n",
      "Epoch 96: train loss: 1.1017866134643555\n",
      "Epoch 97: train loss: 1.1028778553009033\n",
      "Epoch 98: train loss: 1.1002637147903442\n",
      "Epoch 99: train loss: 1.0973517894744873\n",
      "256\n",
      "Epoch 0: train loss: 1.0906838178634644\n",
      "Epoch 1: train loss: 1.9528484344482422\n",
      "Epoch 2: train loss: 1.4080995321273804\n",
      "Epoch 3: train loss: 1.4023833274841309\n",
      "Epoch 4: train loss: 1.3880482912063599\n",
      "Epoch 5: train loss: 1.3394323587417603\n",
      "Epoch 6: train loss: 1.304132342338562\n",
      "Epoch 7: train loss: 1.2907027006149292\n",
      "Epoch 8: train loss: 1.2867493629455566\n",
      "Epoch 9: train loss: 1.2758479118347168\n",
      "Epoch 10: train loss: 1.2543426752090454\n",
      "Epoch 11: train loss: 1.2352811098098755\n",
      "Epoch 12: train loss: 1.2231178283691406\n",
      "Epoch 13: train loss: 1.2224856615066528\n",
      "Epoch 14: train loss: 1.222825288772583\n",
      "Epoch 15: train loss: 1.2131943702697754\n",
      "Epoch 16: train loss: 1.202736258506775\n",
      "Epoch 17: train loss: 1.1960245370864868\n",
      "Epoch 18: train loss: 1.1920047998428345\n",
      "Epoch 19: train loss: 1.1876022815704346\n",
      "Epoch 20: train loss: 1.1781063079833984\n",
      "Epoch 21: train loss: 1.1700234413146973\n",
      "Epoch 22: train loss: 1.1655222177505493\n",
      "Epoch 23: train loss: 1.1588709354400635\n",
      "Epoch 24: train loss: 1.152270793914795\n",
      "Epoch 25: train loss: 1.1481366157531738\n",
      "Epoch 26: train loss: 1.1421197652816772\n",
      "Epoch 27: train loss: 1.1379363536834717\n",
      "Epoch 28: train loss: 1.13252592086792\n",
      "Epoch 29: train loss: 1.12720787525177\n",
      "Epoch 30: train loss: 1.1220470666885376\n",
      "Epoch 31: train loss: 1.1172895431518555\n",
      "Epoch 32: train loss: 1.1123374700546265\n",
      "Epoch 33: train loss: 1.1082829236984253\n",
      "Epoch 34: train loss: 1.1042759418487549\n",
      "Epoch 35: train loss: 1.1001875400543213\n",
      "Epoch 36: train loss: 1.095747709274292\n",
      "Epoch 37: train loss: 1.0913841724395752\n",
      "Epoch 38: train loss: 1.0874581336975098\n",
      "Epoch 39: train loss: 1.0836668014526367\n",
      "Epoch 40: train loss: 1.0840851068496704\n",
      "Epoch 41: train loss: 1.087097406387329\n",
      "Epoch 42: train loss: 1.0746917724609375\n",
      "Epoch 43: train loss: 1.0710899829864502\n",
      "Epoch 44: train loss: 1.0715290307998657\n",
      "Epoch 45: train loss: 1.0621230602264404\n",
      "Epoch 46: train loss: 1.0643481016159058\n",
      "Epoch 47: train loss: 1.0594148635864258\n",
      "Epoch 48: train loss: 1.059971570968628\n",
      "Epoch 49: train loss: 1.0587050914764404\n",
      "Epoch 50: train loss: 1.0556026697158813\n",
      "Epoch 51: train loss: 1.049125075340271\n",
      "Epoch 52: train loss: 1.0461899042129517\n",
      "Epoch 53: train loss: 1.0413624048233032\n",
      "Epoch 54: train loss: 1.041447639465332\n",
      "Epoch 55: train loss: 1.0437681674957275\n",
      "Epoch 56: train loss: 1.0424295663833618\n",
      "Epoch 57: train loss: 1.0344364643096924\n",
      "Epoch 58: train loss: 1.0306283235549927\n",
      "Epoch 59: train loss: 1.0319077968597412\n",
      "Epoch 60: train loss: 1.0356897115707397\n",
      "Epoch 61: train loss: 1.025972843170166\n",
      "Epoch 62: train loss: 1.0345181226730347\n",
      "Epoch 63: train loss: 1.0451143980026245\n",
      "Epoch 64: train loss: 1.036225438117981\n",
      "Epoch 65: train loss: 1.0374982357025146\n",
      "Epoch 66: train loss: 1.0201526880264282\n",
      "Epoch 67: train loss: 1.0330595970153809\n",
      "Epoch 68: train loss: 1.019036889076233\n",
      "Epoch 69: train loss: 1.019994854927063\n",
      "Epoch 70: train loss: 1.0189114809036255\n",
      "Epoch 71: train loss: 1.0099575519561768\n",
      "Epoch 72: train loss: 1.0166680812835693\n",
      "Epoch 73: train loss: 1.0041594505310059\n",
      "Epoch 74: train loss: 1.0095192193984985\n",
      "Epoch 75: train loss: 1.0018322467803955\n",
      "Epoch 76: train loss: 1.0006656646728516\n",
      "Epoch 77: train loss: 0.9986671805381775\n",
      "Epoch 78: train loss: 0.9954289793968201\n",
      "Epoch 79: train loss: 0.994137704372406\n",
      "Epoch 80: train loss: 0.9895920157432556\n",
      "Epoch 81: train loss: 0.9892253279685974\n",
      "Epoch 82: train loss: 0.9842216968536377\n",
      "Epoch 83: train loss: 0.9837418794631958\n",
      "Epoch 84: train loss: 0.9808977842330933\n",
      "Epoch 85: train loss: 0.9881281852722168\n",
      "Epoch 86: train loss: 0.9926379323005676\n",
      "Epoch 87: train loss: 0.994222104549408\n",
      "Epoch 88: train loss: 0.9774691462516785\n",
      "Epoch 89: train loss: 0.9855430126190186\n",
      "Epoch 90: train loss: 0.996605396270752\n",
      "Epoch 91: train loss: 0.9822998642921448\n",
      "Epoch 92: train loss: 0.9868736267089844\n",
      "Epoch 93: train loss: 0.973481297492981\n",
      "Epoch 94: train loss: 0.974006712436676\n",
      "Epoch 95: train loss: 0.9692962765693665\n",
      "Epoch 96: train loss: 0.96952885389328\n",
      "Epoch 97: train loss: 0.963241696357727\n",
      "Epoch 98: train loss: 0.9638437628746033\n",
      "Epoch 99: train loss: 0.9579035043716431\n",
      "Gap level 3\n",
      "Training ResNet\n",
      "0.1\n",
      "128\n",
      "Epoch 0: train loss: 1.6888048648834229\n",
      "Epoch 1: train loss: 1.9525091648101807\n",
      "Epoch 2: train loss: 1.8218600749969482\n",
      "Epoch 3: train loss: 1.6735115051269531\n",
      "Epoch 4: train loss: 1.461430311203003\n",
      "Epoch 5: train loss: 1.4228028059005737\n",
      "Epoch 6: train loss: 1.4125858545303345\n",
      "Epoch 7: train loss: 1.4033939838409424\n",
      "Epoch 8: train loss: 1.4150416851043701\n",
      "Epoch 9: train loss: 1.4164810180664062\n",
      "Epoch 10: train loss: 1.40947425365448\n",
      "Epoch 11: train loss: 1.402597188949585\n",
      "Epoch 12: train loss: 1.4018206596374512\n",
      "Epoch 13: train loss: 1.400729775428772\n",
      "Epoch 14: train loss: 1.3965959548950195\n",
      "Epoch 15: train loss: 1.3906806707382202\n",
      "Epoch 16: train loss: 1.3850822448730469\n",
      "Epoch 17: train loss: 1.3826144933700562\n",
      "Epoch 18: train loss: 1.3839383125305176\n",
      "Epoch 19: train loss: 1.3860795497894287\n",
      "Epoch 20: train loss: 1.3870997428894043\n",
      "Epoch 21: train loss: 1.3873847723007202\n",
      "Epoch 22: train loss: 1.387083888053894\n",
      "Epoch 23: train loss: 1.385860800743103\n",
      "Epoch 24: train loss: 1.384432077407837\n",
      "Epoch 25: train loss: 1.3830947875976562\n",
      "Epoch 26: train loss: 1.3821872472763062\n",
      "Epoch 27: train loss: 1.381575345993042\n",
      "Epoch 28: train loss: 1.381491780281067\n",
      "Epoch 29: train loss: 1.381706953048706\n",
      "Epoch 30: train loss: 1.3812570571899414\n",
      "Epoch 31: train loss: 1.3808003664016724\n",
      "Epoch 32: train loss: 1.3804436922073364\n",
      "Epoch 33: train loss: 1.37946355342865\n",
      "Epoch 34: train loss: 1.378725290298462\n",
      "Epoch 35: train loss: 1.3777482509613037\n",
      "Epoch 36: train loss: 1.3771250247955322\n",
      "Epoch 37: train loss: 1.3765116930007935\n",
      "Epoch 38: train loss: 1.376123070716858\n",
      "Epoch 39: train loss: 1.3756797313690186\n",
      "Epoch 40: train loss: 1.3752872943878174\n",
      "Epoch 41: train loss: 1.3749960660934448\n",
      "Epoch 42: train loss: 1.3744245767593384\n",
      "Epoch 43: train loss: 1.3738946914672852\n",
      "Epoch 44: train loss: 1.3731986284255981\n",
      "Epoch 45: train loss: 1.3723700046539307\n",
      "Epoch 46: train loss: 1.3711247444152832\n",
      "Epoch 47: train loss: 1.3698798418045044\n",
      "Epoch 48: train loss: 1.368692398071289\n",
      "Epoch 49: train loss: 1.3676608800888062\n",
      "Epoch 50: train loss: 1.366106390953064\n",
      "Epoch 51: train loss: 1.3646817207336426\n",
      "Epoch 52: train loss: 1.3631978034973145\n",
      "Epoch 53: train loss: 1.3618754148483276\n",
      "Epoch 54: train loss: 1.3607596158981323\n",
      "Epoch 55: train loss: 1.3597979545593262\n",
      "Epoch 56: train loss: 1.3589510917663574\n",
      "Epoch 57: train loss: 1.3580143451690674\n",
      "Epoch 58: train loss: 1.3568029403686523\n",
      "Epoch 59: train loss: 1.3554600477218628\n",
      "Epoch 60: train loss: 1.3539804220199585\n",
      "Epoch 61: train loss: 1.3525395393371582\n",
      "Epoch 62: train loss: 1.3511120080947876\n",
      "Epoch 63: train loss: 1.3496592044830322\n",
      "Epoch 64: train loss: 1.3480533361434937\n",
      "Epoch 65: train loss: 1.3462454080581665\n",
      "Epoch 66: train loss: 1.3442320823669434\n",
      "Epoch 67: train loss: 1.342028260231018\n",
      "Epoch 68: train loss: 1.339538812637329\n",
      "Epoch 69: train loss: 1.3364821672439575\n",
      "Epoch 70: train loss: 1.332861304283142\n",
      "Epoch 71: train loss: 1.329397439956665\n",
      "Epoch 72: train loss: 1.3269643783569336\n",
      "Epoch 73: train loss: 1.3253933191299438\n",
      "Epoch 74: train loss: 1.324742078781128\n",
      "Epoch 75: train loss: 1.3268967866897583\n",
      "Epoch 76: train loss: 1.3228362798690796\n",
      "Epoch 77: train loss: 1.3175113201141357\n",
      "Epoch 78: train loss: 1.3164730072021484\n",
      "Epoch 79: train loss: 1.3172048330307007\n",
      "Epoch 80: train loss: 1.3152079582214355\n",
      "Epoch 81: train loss: 1.3111793994903564\n",
      "Epoch 82: train loss: 1.3121274709701538\n",
      "Epoch 83: train loss: 1.3088494539260864\n",
      "Epoch 84: train loss: 1.305737853050232\n",
      "Epoch 85: train loss: 1.3060110807418823\n",
      "Epoch 86: train loss: 1.301397442817688\n",
      "Epoch 87: train loss: 1.3000102043151855\n",
      "Epoch 88: train loss: 1.2983849048614502\n",
      "Epoch 89: train loss: 1.2940436601638794\n",
      "Epoch 90: train loss: 1.2917507886886597\n",
      "Epoch 91: train loss: 1.2897114753723145\n",
      "Epoch 92: train loss: 1.2864454984664917\n",
      "Epoch 93: train loss: 1.282289743423462\n",
      "Epoch 94: train loss: 1.279039978981018\n",
      "Epoch 95: train loss: 1.2754812240600586\n",
      "Epoch 96: train loss: 1.2744472026824951\n",
      "Epoch 97: train loss: 1.2869949340820312\n",
      "Epoch 98: train loss: 1.2679133415222168\n",
      "Epoch 99: train loss: 1.2879303693771362\n",
      "256\n",
      "Epoch 0: train loss: 1.2887221574783325\n",
      "Epoch 1: train loss: 1.5855733156204224\n",
      "Epoch 2: train loss: 1.4734954833984375\n",
      "Epoch 3: train loss: 1.3962678909301758\n",
      "Epoch 4: train loss: 1.3849436044692993\n",
      "Epoch 5: train loss: 1.370648980140686\n",
      "Epoch 6: train loss: 1.3399598598480225\n",
      "Epoch 7: train loss: 1.3319344520568848\n",
      "Epoch 8: train loss: 1.3325291872024536\n",
      "Epoch 9: train loss: 1.333255648612976\n",
      "Epoch 10: train loss: 1.3322187662124634\n",
      "Epoch 11: train loss: 1.3278228044509888\n",
      "Epoch 12: train loss: 1.3218717575073242\n",
      "Epoch 13: train loss: 1.31618332862854\n",
      "Epoch 14: train loss: 1.3104861974716187\n",
      "Epoch 15: train loss: 1.3055834770202637\n",
      "Epoch 16: train loss: 1.302474856376648\n",
      "Epoch 17: train loss: 1.2984920740127563\n",
      "Epoch 18: train loss: 1.2922362089157104\n",
      "Epoch 19: train loss: 1.284288763999939\n",
      "Epoch 20: train loss: 1.2762867212295532\n",
      "Epoch 21: train loss: 1.2682465314865112\n",
      "Epoch 22: train loss: 1.2592859268188477\n",
      "Epoch 23: train loss: 1.2494570016860962\n",
      "Epoch 24: train loss: 1.2400919198989868\n",
      "Epoch 25: train loss: 1.2322639226913452\n",
      "Epoch 26: train loss: 1.2249531745910645\n",
      "Epoch 27: train loss: 1.2171344757080078\n",
      "Epoch 28: train loss: 1.2087116241455078\n",
      "Epoch 29: train loss: 1.1996296644210815\n",
      "Epoch 30: train loss: 1.1928589344024658\n",
      "Epoch 31: train loss: 1.1896288394927979\n",
      "Epoch 32: train loss: 1.1860995292663574\n",
      "Epoch 33: train loss: 1.1686334609985352\n",
      "Epoch 34: train loss: 1.1708027124404907\n",
      "Epoch 35: train loss: 1.1555960178375244\n",
      "Epoch 36: train loss: 1.1539379358291626\n",
      "Epoch 37: train loss: 1.1393308639526367\n",
      "Epoch 38: train loss: 1.1431159973144531\n",
      "Epoch 39: train loss: 1.1280113458633423\n",
      "Epoch 40: train loss: 1.1298189163208008\n",
      "Epoch 41: train loss: 1.1268775463104248\n",
      "Epoch 42: train loss: 1.119348168373108\n",
      "Epoch 43: train loss: 1.1150020360946655\n",
      "Epoch 44: train loss: 1.1077611446380615\n",
      "Epoch 45: train loss: 1.1028097867965698\n",
      "Epoch 46: train loss: 1.0993868112564087\n",
      "Epoch 47: train loss: 1.0937656164169312\n",
      "Epoch 48: train loss: 1.09148108959198\n",
      "Epoch 49: train loss: 1.0850774049758911\n",
      "Epoch 50: train loss: 1.083897590637207\n",
      "Epoch 51: train loss: 1.078506588935852\n",
      "Epoch 52: train loss: 1.0748552083969116\n",
      "Epoch 53: train loss: 1.0717153549194336\n",
      "Epoch 54: train loss: 1.069880723953247\n",
      "Epoch 55: train loss: 1.065819501876831\n",
      "Epoch 56: train loss: 1.0662062168121338\n",
      "Epoch 57: train loss: 1.0649055242538452\n",
      "Epoch 58: train loss: 1.0626157522201538\n",
      "Epoch 59: train loss: 1.0528825521469116\n",
      "Epoch 60: train loss: 1.0464085340499878\n",
      "Epoch 61: train loss: 1.0480047464370728\n",
      "Epoch 62: train loss: 1.0415246486663818\n",
      "Epoch 63: train loss: 1.0375404357910156\n",
      "Epoch 64: train loss: 1.031606912612915\n",
      "Epoch 65: train loss: 1.0297529697418213\n",
      "Epoch 66: train loss: 1.029997706413269\n",
      "Epoch 67: train loss: 1.025701880455017\n",
      "Epoch 68: train loss: 1.024138331413269\n",
      "Epoch 69: train loss: 1.025083303451538\n",
      "Epoch 70: train loss: 1.0247783660888672\n",
      "Epoch 71: train loss: 1.02902090549469\n",
      "Epoch 72: train loss: 1.0180799961090088\n",
      "Epoch 73: train loss: 1.0174579620361328\n",
      "Epoch 74: train loss: 1.0222346782684326\n",
      "Epoch 75: train loss: 1.0051546096801758\n",
      "Epoch 76: train loss: 1.0167121887207031\n",
      "Epoch 77: train loss: 1.0095610618591309\n",
      "Epoch 78: train loss: 1.0081297159194946\n",
      "Epoch 79: train loss: 0.999750018119812\n",
      "Epoch 80: train loss: 1.0012290477752686\n",
      "Epoch 81: train loss: 0.9967000484466553\n",
      "Epoch 82: train loss: 0.9942934513092041\n",
      "Epoch 83: train loss: 0.9895194172859192\n",
      "Epoch 84: train loss: 0.9889368414878845\n",
      "Epoch 85: train loss: 0.9852954149246216\n",
      "Epoch 86: train loss: 0.9860177040100098\n",
      "Epoch 87: train loss: 0.9827005863189697\n",
      "Epoch 88: train loss: 0.9792349934577942\n",
      "Epoch 89: train loss: 0.9763633012771606\n",
      "Epoch 90: train loss: 0.9732086062431335\n",
      "Epoch 91: train loss: 0.9704409241676331\n",
      "Epoch 92: train loss: 0.9676600694656372\n",
      "Epoch 93: train loss: 0.9668449759483337\n",
      "Epoch 94: train loss: 0.965621292591095\n",
      "Epoch 95: train loss: 0.9760145545005798\n",
      "Epoch 96: train loss: 0.9931325316429138\n",
      "Epoch 97: train loss: 1.0155839920043945\n",
      "Epoch 98: train loss: 0.9740186333656311\n",
      "Epoch 99: train loss: 1.0017420053482056\n"
     ]
    }
   ],
   "source": [
    "### Training loop\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "\n",
    "\n",
    "# Input data\n",
    "#data_path = \"C:/Users\\lukec\\PycharmProjects\\emissions-tracking-conda\\emissions-tracking\\data\\classification_inputs/\"\n",
    "for input_data in ['petrochemicals']:\n",
    "    print(input_data)\n",
    "     # Output data\n",
    "    #data_path = 'C:/Users\\lukec\\PycharmProjects\\emissions-tracking-conda\\emissions-tracking\\models/datasets/'\n",
    "    model_path = \"/content/drive/MyDrive/PhD - Climate Change Mitigation/Outputs/\"+input_data+'/'\n",
    "    #'C:/Users\\lukec\\PycharmProjects\\emissions-tracking-conda\\emissions-tracking\\models/'+input_data+'/' # '/content/models/'\n",
    "    balance=False\n",
    "\n",
    "    learning_rates = [0.001, 0.01, 0.1]\n",
    "    hidden_sizes = [64, 128, 256]\n",
    "    gap_filling_levels = [1,2,3]\n",
    "\n",
    "    # Define divider for level 3\n",
    "    if input_data == 'CT_manufacturing':\n",
    "        divider = 'iso3_country'\n",
    "        inference_cols = ['iso3_country', 'original_inventory_sector', 'asset_type']\n",
    "        time_col = 'Timestep'\n",
    "        timesteps = [str(i) for i in range(0,90)]\n",
    "        graph_cols, max_edges = [0,1,3,5], 100\n",
    "\n",
    "    elif input_data == 'petrochemicals':\n",
    "        divider = 'COUNTRY/TERRITORY'\n",
    "        inference_cols = ['PRODUCT', 'COUNTRY/TERRITORY']\n",
    "        time_col = 'Year'\n",
    "        timesteps = [str(i) for i in range(1978,2051)]\n",
    "        graph_cols, max_edges = [0,3], 30\n",
    "        balance = True\n",
    "\n",
    "    elif input_data == 'unfccc':\n",
    "        divider='Party'\n",
    "        inference_cols = ['Party', 'Category']\n",
    "        time_col = 'Year'\n",
    "        timesteps = [str(i) for i in range(1990,2021)]\n",
    "        graph_cols, max_edges = [0], 100\n",
    "\n",
    "    ## Parameters\n",
    "    max_test_set = 100000\n",
    "    random_state = 2\n",
    "    test_size = 0.3\n",
    "    regression = False\n",
    "\n",
    "    # Gap level\n",
    "    for gap_filling_level in gap_filling_levels:\n",
    "        print('Gap level '+str(gap_filling_level))\n",
    "        X_train_pt = torch.load(data_path+'X_train_pt-'+input_data+'-'+str(gap_filling_level)+'.pt')\n",
    "        X_test_pt = torch.load(data_path+'X_test_pt-'+input_data+'-'+str(gap_filling_level)+'.pt')\n",
    "        y_train_pt = torch.load(data_path+'y_train_pt-'+input_data+'-'+str(gap_filling_level)+'.pt')\n",
    "        y_test_pt = torch.load(data_path+'y_test_pt-'+input_data+'-'+str(gap_filling_level)+'.pt')\n",
    "\n",
    "\n",
    "        if balance:\n",
    "          X_train_pt, X_test_pt, y_train_pt, y_test_pt = balance_classes_pt(X_train_pt, X_test_pt, y_train_pt, y_test_pt)\n",
    "\n",
    "        X_train_pt, X_test_pt, y_train_pt, y_test_pt = X_train_pt.to('cuda:0'), X_test_pt.to('cuda:0'), y_train_pt.to('cuda:0'), y_test_pt.to('cuda:0')\n",
    "\n",
    "        input_dim = X_train_pt.shape[1]\n",
    "        input_channels = 1#X_train_pt.shape[1]\n",
    "        output_dim = 4\n",
    "        hidden_dim1 = 64\n",
    "        hidden_dim2 = 64\n",
    "\n",
    "\n",
    "        models = [\n",
    "                    (\"DeepNet\", DeepNet(input_dim, hidden_dim1, hidden_dim2, output_dim).to('cuda')),\n",
    "                    ('LSTM', LSTMNet(input_dim, hidden_dim1, output_dim).to('cuda')),\n",
    "                    ('ResNet', ResNet(input_channels, output_dim).to('cuda')),\n",
    "                    #('Transformer', TransformerNet(input_dim,256, output_dim))\n",
    "                ]\n",
    "\n",
    "        # Define a list of models to train\n",
    "\n",
    "        best_accuracy = {}\n",
    "        best_learning_rate = {}\n",
    "        best_hidden_size = {}\n",
    "\n",
    "        for model_name, model in models:\n",
    "\n",
    "            best_accuracy[model_name] = 0\n",
    "            best_learning_rate[model_name] = 0\n",
    "            best_hidden_size[model_name] = 0\n",
    "\n",
    "            print(f\"Training {model_name}\")\n",
    "            for learning_rate in learning_rates:\n",
    "                print(learning_rate)\n",
    "                for hidden_size in hidden_sizes:\n",
    "                    print(hidden_size)\n",
    "                    model_type = 'reg' if regression else 'clf'\n",
    "                    model_file = model_path + model_type + '_' + model_name + '_l' + str(str(gap_filling_level)) + '_' + date.today().strftime(\"%y%m%d\")\n",
    "\n",
    "                    # Set the hyperparameters\n",
    "                    if model_name == 'DeepNet':\n",
    "                        model.hidden_size1 = hidden_size\n",
    "                        model.hidden_size2 = hidden_size\n",
    "                    if model_name == 'LSTM':\n",
    "                        model.hidden_dim1 = hidden_size\n",
    "                    # Reshape the input to have 4 dimensions\n",
    "                    if model_name == 'LSTM' or model_name=='ResNet':\n",
    "                        # Reshape the input data to have 3 channels\n",
    "                        X_train_in = X_train_pt.unsqueeze(1).to('cuda:0')\n",
    "                        X_test_in = X_test_pt.unsqueeze(1).to('cuda:0')\n",
    "                    else:\n",
    "                        X_train_in = X_train_pt.to('cuda:0')\n",
    "                        X_test_in = X_test_pt.to('cuda:0')\n",
    "\n",
    "\n",
    "                    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "                    criterion = nn.CrossEntropyLoss()\n",
    "                    epochs = 100\n",
    "\n",
    "                    loss, model, optimizer = train(model, X_train_in, y_train_pt, optimizer, criterion, epochs=epochs)\n",
    "\n",
    "                    #torch.save(model.state_dict(), model_file+'.pt')\n",
    "\n",
    "                    if regression:\n",
    "                        y_pred = test(model, X_test_in)\n",
    "                    else:\n",
    "                        _, y_pred = test(model, X_test_in)\n",
    "\n",
    "                    scores = metrics(y_test_pt.cpu().numpy(), y_pred.cpu().numpy(), model_type)\n",
    "\n",
    "                    np.save(model_file+'_'+str(learning_rate)+'_'+str(hidden_size)+'_100iter.npy', scores)\n",
    "\n",
    "        #             # Check if the current combination of hyperparameters is the best\n",
    "                    if scores['average_acc'] > best_accuracy[model_name]:\n",
    "                        torch.save(model.state_dict(), model_file+'_'+str(learning_rate)+'_'+str(hidden_size)+'.pt')\n",
    "                        # best_accuracy[model_name] = scores['average_acc']\n",
    "                        # best_learning_rate[model_name] = learning_rate\n",
    "                        # best_hidden_size[model_name] = hidden_size\n",
    "\n",
    "        # # Train the best models with the best hyperparameters for 100 epochs\n",
    "        # for model_name, model in models:\n",
    "        #     best_lr = best_learning_rate[model_name]\n",
    "        #     best_hs = best_hidden_size[model_name]\n",
    "        #     model_type = 'reg' if regression else 'clf'\n",
    "        #     model_file = model_path + model_type + '_' + model_name + '_l' + str(str(gap_filling_level)) + '_' + date.today().strftime(\"%y%m%d\")\n",
    "\n",
    "        #     # Set the hyperparameters\n",
    "        #     if model_name == 'DeepNet':\n",
    "        #         model.hidden_size1 = best_hs\n",
    "        #         model.hidden_size2 = best_hs\n",
    "        #     if model_name == 'LSTM':\n",
    "        #         model.hidden_dim1 = best_hs\n",
    "        #     # Reshape the input to have 4 dimensions\n",
    "        #     if model_name == 'LSTM' or model_name=='ResNet':\n",
    "        #         # Reshape the input data to have 3 channels\n",
    "        #         X_train_in = X_train_pt.unsqueeze(1).to('cuda:0')\n",
    "        #         X_test_in = X_test_pt.unsqueeze(1).to('cuda:0')\n",
    "        #     else:\n",
    "        #         X_train_in = X_train_pt.to('cuda:0')\n",
    "        #         X_test_in = X_test_pt.to('cuda:0')\n",
    "\n",
    "\n",
    "        #     optimizer = torch.optim.Adam(model.parameters(), lr=best_lr)\n",
    "        #     criterion = nn.CrossEntropyLoss()\n",
    "        #     epochs = 100\n",
    "\n",
    "        #     #model = model\n",
    "        #     loss, model, optimizer = train(model, X_train_in, y_train_pt, optimizer, criterion, epochs=epochs)\n",
    "\n",
    "        #     torch.save(model.state_dict(), model_file+'.pt')\n",
    "\n",
    "        #     if regression:\n",
    "        #         y_pred = test(model, X_test_in)\n",
    "        #     else:\n",
    "        #         _, y_pred = test(model, X_test_in)\n",
    "\n",
    "        #     scores = metrics(y_test_pt, y_pred, model_type)\n",
    "\n",
    "        #     np.save(model_file+'_'+str(best_lr)+'_'+str(best_hs)+'_100iter.npy', scores)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "SdizRfxbrxRJ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "eecc8fb4-21e3-49b5-e279-207fa86ae28c"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "id": "dFjdRFUfWo0P"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "KuVbnDbUcJ9n"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}