{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "#!pip install torch_sparse"
   ],
   "metadata": {
    "id": "MGbECsd4zgcH"
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "import sklearn.linear_model\n",
    "\n",
    "\"\"\"Define graph forming functions\"\"\"\n",
    "# Import packages\n",
    "\n",
    "import math\n",
    "import joblib\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "#from google.colab import files\n",
    "\n",
    "from sklearn import tree\n",
    "import sklearn.svm as svm\n",
    "from sklearn import ensemble\n",
    "import sklearn.metrics as skm\n",
    "import sklearn.preprocessing as pp\n",
    "import sklearn.linear_model as lms\n",
    "import sklearn.neural_network as skl_nn\n",
    "import sklearn.neighbors as neighbors\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import sklearn.model_selection as model_sel\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch_sparse\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data\n",
    "# from torch_geometric.nn import GCNConv"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "oEBsNP4nzFhS"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Input data\n",
    "random_state = 2\n",
    "np.random.seed(random_state)\n",
    "data_path = 'C:/Users\\lukec\\PycharmProjects\\emissions-tracking-conda\\emissions-tracking\\models\\datasets/'\n",
    "#\"/content/datasets/\"\n",
    "\n",
    "max_test_set = 100000"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "NFUHGM_xzFhV"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def filter_for_start_yr(df, start_col, end_col) -> pd.DataFrame:\n",
    "    \"\"\"Convert dataframe of plants with entry for each year into dataframe with row for each year\"\"\"\n",
    "    # Get rid of emissions for years before start year\n",
    "    df['Age'] = df['Year'].astype(int) - df[start_col].astype(int)\n",
    "    df = df[df['Age'] >= 0]\n",
    "\n",
    "    # Get rid of emissions for years after end years\n",
    "    df['ToGo'] = df[end_col].astype(int) - df['Year'].astype(int)\n",
    "    df = df[df['ToGo'] >= 0]\n",
    "\n",
    "    df[['START_YR', 'END_YR']] = df[['START_YR', 'END_YR']].astype(int)\n",
    "\n",
    "    return df.drop(columns=['ToGo'])\n",
    "\n",
    "def pivot_data_dfs(df:pd.DataFrame, time_col) -> [pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Pivot data dataframes to get each entry and all year/month values in rows\"\"\"\n",
    "    feature_cols = [i for i in df.columns if i not in [time_col, 'Emissions']]\n",
    "    df_pivoted = df.pivot(index=feature_cols,\n",
    "                          columns=time_col,\n",
    "                          values = 'Emissions').reset_index()\n",
    "    return df_pivoted, feature_cols\n",
    "\n",
    "\n",
    "def melt_data_dfs(df:pd.DataFrame, feature_cols, time_col) -> pd.DataFrame:\n",
    "    \"\"\"Melt data dataframes to get row per date entry for each facility\"\"\"\n",
    "    melted = df.melt(id_vars=feature_cols, var_name=time_col, value_name='Emissions').dropna(subset=['Emissions'])\n",
    "    melted[time_col] = melted[time_col].astype(int)\n",
    "    \n",
    "    if 'START_YR' in df.columns and 'END_YR' in df.columns:\n",
    "        melted = filter_for_start_yr(melted, 'START_YR', 'END_YR')\n",
    "\n",
    "    return melted\n",
    "\n",
    "## Convert train and test sets into ML ready sets\n",
    "def series_to_bins(series:pd.Series, bins:list=None, labels:list=None, positive:bool=True):\n",
    "    # Convert a continuous pandas dataframe column into discrete bins\n",
    "    if bins is None:\n",
    "        bin_series = series[series!=0] if positive else series\n",
    "        bins = [min(bin_series.min(),0)-0.01, bin_series.quantile(0.25), bin_series.quantile(0.5), bin_series.quantile(0.75), bin_series.max()+0.01]\n",
    "    if labels is None: labels = list(range(len(bins)-1))\n",
    "\n",
    "    transformer = pp.FunctionTransformer(\n",
    "        pd.cut, kw_args={'bins': bins, 'labels': labels, 'retbins': False}\n",
    "    )\n",
    "    return bins, transformer.fit_transform(series)\n",
    "\n",
    "\n",
    "def preprocess_yearly(train_set, test_set, y_col='Emissions'):\n",
    "    \"\"\"Digitise train and test sets\"\"\"\n",
    "\n",
    "    # Create Y\n",
    "    bins, y_train_clf = series_to_bins(train_set[y_col])\n",
    "    _, y_test_clf = series_to_bins(test_set[y_col], bins=[test_set[y_col].min()-0.01]+bins[1:-1]+[test_set[y_col].max()+0.01])\n",
    "\n",
    "    y_train_reg, y_test_reg = train_set[y_col], test_set[y_col]\n",
    "    train_set, test_set = train_set.drop(columns=[y_col]), test_set.drop(columns=[y_col])\n",
    "\n",
    "    # Create X\n",
    "    # Deal with string columns\n",
    "    x_enc = pp.OrdinalEncoder()\n",
    "    string_cols = list(train_set.select_dtypes(include='object').columns)\n",
    "    train_set[string_cols] = train_set[string_cols].astype(str)\n",
    "    test_set[string_cols] = test_set[string_cols].astype(str)\n",
    "    x_strings = pd.concat((train_set[string_cols], test_set[string_cols]))\n",
    "    x_enc.fit(x_strings)\n",
    "\n",
    "    # Make float columns into int columns\n",
    "    float_cols = list(train_set.select_dtypes(include='float').columns)\n",
    "    train_set[float_cols], test_set[float_cols] = train_set[float_cols].astype(int), test_set[float_cols].astype(int)\n",
    "\n",
    "    if 'LATITUDE' in list(train_set.columns) and 'LONGITUDE' in list(train_set.columns):\n",
    "        train_set[['LATITUDE', 'LONGITUDE']] = (train_set[['LATITUDE', 'LONGITUDE']].astype(int)+[90, 180])\n",
    "        test_set[['LATITUDE', 'LONGITUDE']] = (test_set[['LATITUDE', 'LONGITUDE']].astype(int)+[90, 180])\n",
    "\n",
    "    int_cols = list(train_set.select_dtypes(include='integer').columns)\n",
    "    x_ints_min = pd.concat((train_set[int_cols], test_set[int_cols])).min().values\n",
    "    x_ints_train = train_set[int_cols] - x_ints_min\n",
    "    x_ints_test = test_set[int_cols] - x_ints_min\n",
    "\n",
    "    X_train = np.concatenate((x_enc.transform(train_set[string_cols]),\n",
    "                              x_ints_train.values), axis=1)\n",
    "    X_test = np.concatenate((x_enc.transform(test_set[string_cols]),\n",
    "                              x_ints_test.values), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    return X_train, X_test, y_train_clf, y_test_clf, y_train_reg, y_test_reg, x_enc\n",
    "\n",
    "def save_decoded_X(X, x_enc, cols, used, name):\n",
    "    min_years = [used['START_YR'].astype(int).min(), 1978]\n",
    "    X_inv = np.concatenate((x_enc.inverse_transform(X[:,:-4]), (X[:,-4:-2]+min_years).astype(int), X[:,-2:]), axis=1)\n",
    "    pd.DataFrame(X_inv, columns=list(columns[:-2]+['Year']+columns[-2:])).to_csv(name+'.csv')\n",
    "\n",
    "# Function to split rows into two DataFrames\n",
    "def split_rows(group, test_fraction):\n",
    "\n",
    "    num_rows = group.shape[0]\n",
    "    if num_rows == 1:\n",
    "        return None, None  # Exclude groups with only one sample\n",
    "\n",
    "    num_sampled_rows = int(min(max(test_fraction * num_rows, 1), num_rows-1))  # At least one sample for each group\n",
    "\n",
    "    test_df = group.sample(n=num_sampled_rows)\n",
    "    train_df = group.drop(test_df.index)\n",
    "\n",
    "    return train_df, test_df"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "L0-1GPoPzFhW"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def custom_interpolate(row):\n",
    "    if row.count() >= 3:  # Check if there are enough values for polynomial interpolation\n",
    "        return row.interpolate(method='polynomial', order=order, limit_direction='both')\n",
    "    else:\n",
    "        return row.interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "def metrics(y_true, y_pred, model_type='clf'):\n",
    "    if model_type == 'clf':\n",
    "        metric_dict = {'confusion': skm.confusion_matrix(y_true, y_pred),\n",
    "                       'overall_acc': skm.accuracy_score(y_true, y_pred),\n",
    "                       'average_acc': skm.balanced_accuracy_score(y_true, y_pred),\n",
    "                       'kappa': skm.cohen_kappa_score(y_true, y_pred),\n",
    "                       'IoU': skm.jaccard_score(y_true, y_pred, average='weighted')}\n",
    "    elif model_type == 'reg':\n",
    "        metric_dict = {'r2': skm.r2_score(y_true, y_pred),\n",
    "                       'mae': skm.mean_absolute_error(y_true, y_pred),\n",
    "                       'mse': skm.mean_squared_error(y_true, y_pred)}\n",
    "\n",
    "    else: raise 'Incorrect model type'\n",
    "\n",
    "    return metric_dict\n",
    "\n",
    "def pd_to_adj_matrix(df:pd.DataFrame, columns:list, weights:list = False, remove_self_conns:bool = True, max_edges = 100, verbose:bool = True):\n",
    "    \"\"\"Form pytorch COO format adjacency matrix from pandas dataframe columns\"\"\"\n",
    "    groups = [group.index.values.astype(int) for col in columns for _, group in df.groupby(col)]\n",
    "\n",
    "    rows = torch.tensor(np.concatenate([np.tile(g.flatten(), min(len(g), max_edges)) for g in groups]), dtype=torch.long)\n",
    "    cols = torch.tensor(np.concatenate([np.repeat(g.flatten(), min(len(g), max_edges)) for g in groups]), dtype=torch.long)\n",
    "\n",
    "    if weights is False:\n",
    "        weight_vector = torch.tensor(np.ones(len(rows), dtype=int), dtype=torch.float)\n",
    "    else: weight_vector = torch.tensor(np.ones(len(rows), dtype=int)*weights, dtype=torch.long)\n",
    "\n",
    "    adjacency = torch_sparse.SparseTensor(row=rows, col=cols, value=weight_vector)\n",
    "\n",
    "    return adjacency\n",
    "\n",
    "\n",
    "def balance_classes_pt(X_train, X_test, y_train, y_test, col_name = 'Emissions', X_train_unscaled=False, X_test_unscaled=False):\n",
    "    y_train_pd = pd.Series(y_train, name=col_name)\n",
    "    min_count = y_train_pd.reset_index().groupby(col_name).count().min()\n",
    "    y_train_df = y_train_pd.reset_index().groupby(col_name).sample(min_count.values)\n",
    "    y_train = y_train_df[col_name].values\n",
    "    X_train = X_train[y_train_df.index]\n",
    "\n",
    "    y_test_pd = pd.Series(y_test, name=col_name)\n",
    "    min_count = y_test_pd.reset_index().groupby(col_name).count().min()\n",
    "    y_test_df = y_test_pd.reset_index().groupby(col_name).sample(min_count.values)\n",
    "    y_test = y_test_df[col_name].values\n",
    "    X_test = X_test[y_test_df.index]\n",
    "\n",
    "    if X_train_unscaled is not False:\n",
    "      X_train_unscaled = X_train_unscaled[y_train_df.index]\n",
    "      X_test_unscaled = X_test_unscaled[y_test_df.index]\n",
    "      return X_train, X_test, torch.tensor(y_train), torch.tensor(y_test), X_train_unscaled, X_test_unscaled\n",
    "    else:\n",
    "      return X_train, X_test, torch.tensor(y_train), torch.tensor(y_test)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "u4mIUn4vzFhY"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CT_manufacturing\n",
      "Gap level 1\n",
      "64\n",
      "Training GCLSTM\n",
      "0.01\n",
      "in\n",
      "Train activated\n",
      "0\n",
      "zeroed\n",
      "out\n",
      "loss\n",
      "Epoch 0: train loss: 1.3850598335266113\n",
      "1\n",
      "zeroed\n",
      "out\n",
      "loss\n",
      "Epoch 1: train loss: 1.3434926271438599\n",
      "2\n",
      "zeroed\n",
      "out\n",
      "loss\n",
      "Epoch 2: train loss: 1.2638555765151978\n",
      "3\n",
      "zeroed\n",
      "out\n",
      "loss\n",
      "Epoch 3: train loss: 1.0503066778182983\n",
      "4\n",
      "zeroed\n",
      "out\n",
      "loss\n",
      "Epoch 4: train loss: 0.7897329330444336\n",
      "5\n",
      "zeroed\n",
      "out\n",
      "loss\n",
      "Epoch 5: train loss: 0.6696930527687073\n",
      "6\n",
      "zeroed\n",
      "out\n",
      "loss\n",
      "Epoch 6: train loss: 1.5732266902923584\n",
      "7\n",
      "zeroed\n",
      "out\n",
      "loss\n",
      "Epoch 7: train loss: 0.5507981777191162\n",
      "8\n",
      "zeroed\n",
      "out\n",
      "loss\n",
      "Epoch 8: train loss: 0.48783057928085327\n",
      "9\n",
      "zeroed\n",
      "out\n",
      "loss\n",
      "Epoch 9: train loss: 0.43939098715782166\n",
      "10\n",
      "zeroed\n",
      "out\n",
      "loss\n",
      "Epoch 10: train loss: 0.41586723923683167\n",
      "11\n",
      "zeroed\n",
      "out\n",
      "loss\n",
      "Epoch 11: train loss: 0.4298539459705353\n",
      "12\n",
      "zeroed\n",
      "out\n",
      "loss\n",
      "Epoch 12: train loss: 0.4459995925426483\n",
      "13\n",
      "zeroed\n",
      "out\n",
      "loss\n",
      "Epoch 13: train loss: 0.5271507501602173\n",
      "14\n",
      "zeroed\n",
      "out\n",
      "loss\n",
      "Epoch 14: train loss: 0.37927868962287903\n",
      "15\n",
      "zeroed\n",
      "out\n",
      "loss\n",
      "Epoch 15: train loss: 0.7459855079650879\n",
      "16\n",
      "zeroed\n",
      "out\n",
      "loss\n",
      "Epoch 16: train loss: 0.4026939272880554\n",
      "17\n",
      "zeroed\n",
      "out\n",
      "loss\n",
      "Epoch 17: train loss: 0.5108152627944946\n",
      "18\n",
      "zeroed\n",
      "out\n",
      "loss\n",
      "Epoch 18: train loss: 0.4298733174800873\n",
      "19\n",
      "zeroed\n",
      "out\n",
      "loss\n",
      "Epoch 19: train loss: 0.5145536661148071\n",
      "20\n",
      "zeroed\n",
      "out\n",
      "loss\n",
      "Epoch 20: train loss: 2.050515651702881\n",
      "21\n",
      "zeroed\n",
      "out\n",
      "loss\n",
      "Epoch 21: train loss: 0.39129167795181274\n",
      "22\n",
      "zeroed\n",
      "out\n",
      "loss\n",
      "Epoch 22: train loss: 0.34956616163253784\n",
      "23\n",
      "zeroed\n",
      "out\n",
      "loss\n",
      "Epoch 23: train loss: 0.36211317777633667\n",
      "24\n",
      "zeroed\n",
      "out\n",
      "loss\n",
      "Epoch 24: train loss: 0.28761908411979675\n",
      "25\n",
      "zeroed\n",
      "out\n",
      "loss\n",
      "Epoch 25: train loss: 0.26546424627304077\n",
      "26\n",
      "zeroed\n",
      "out\n",
      "loss\n",
      "Epoch 26: train loss: 0.24076683819293976\n",
      "27\n",
      "zeroed\n",
      "out\n",
      "loss\n",
      "Epoch 27: train loss: 0.21781952679157257\n",
      "28\n",
      "zeroed\n",
      "out\n",
      "loss\n",
      "Epoch 28: train loss: 0.1988786906003952\n",
      "29\n",
      "zeroed\n",
      "out\n",
      "loss\n",
      "Epoch 29: train loss: 0.16343805193901062\n",
      "30\n",
      "zeroed\n",
      "out\n",
      "loss\n",
      "Epoch 30: train loss: 0.1422375738620758\n",
      "31\n",
      "zeroed\n",
      "out\n",
      "loss\n",
      "Epoch 31: train loss: 0.15349631011486053\n",
      "32\n",
      "zeroed\n",
      "out\n",
      "loss\n",
      "Epoch 32: train loss: 0.156304731965065\n",
      "33\n",
      "zeroed\n",
      "out\n",
      "loss\n",
      "Epoch 33: train loss: 0.10730282962322235\n",
      "34\n",
      "zeroed\n",
      "out\n",
      "loss\n",
      "Epoch 34: train loss: 0.4956020712852478\n",
      "35\n",
      "zeroed\n",
      "out\n",
      "loss\n",
      "Epoch 35: train loss: 0.7652705311775208\n",
      "36\n",
      "zeroed\n",
      "out\n",
      "loss\n",
      "Epoch 36: train loss: 0.6884235739707947\n",
      "37\n",
      "zeroed\n",
      "out\n",
      "loss\n",
      "Epoch 37: train loss: 0.5719847083091736\n",
      "38\n",
      "zeroed\n",
      "out\n",
      "loss\n",
      "Epoch 38: train loss: 0.33075159788131714\n",
      "39\n",
      "zeroed\n",
      "out\n",
      "loss\n",
      "Epoch 39: train loss: 0.133779376745224\n",
      "40\n",
      "zeroed\n",
      "out\n",
      "loss\n",
      "Epoch 40: train loss: 0.19009113311767578\n",
      "41\n",
      "zeroed\n",
      "out\n",
      "loss\n",
      "Epoch 41: train loss: 0.23245418071746826\n",
      "42\n",
      "zeroed\n",
      "out\n",
      "loss\n",
      "Epoch 42: train loss: 0.29119813442230225\n",
      "43\n",
      "zeroed\n",
      "out\n",
      "loss\n",
      "Epoch 43: train loss: 0.24797098338603973\n"
     ]
    }
   ],
   "source": [
    "## Create all graphs\n",
    "learning_rates = [0.01]\n",
    "hidden_sizes = [64]\n",
    "\n",
    "balance=True\n",
    "for input_data in ['CT_manufacturing','petrochemicals','unfccc']:\n",
    "    print(input_data)\n",
    "     # Output data\n",
    "    #data_path = 'C:/Users\\lukec\\PycharmProjects\\emissions-tracking-conda\\emissions-tracking\\models/datasets/'\n",
    "    model_path = 'C:/Users\\lukec\\PycharmProjects\\emissions-tracking-conda\\emissions-tracking\\models/'+input_data+'/' # '/content/models/'\n",
    "\n",
    "    # # Define divider for level 3\n",
    "    if input_data == 'CT_manufacturing':\n",
    "        divider = 'iso3_country'\n",
    "        inference_cols = ['iso3_country', 'original_inventory_sector', 'asset_type']\n",
    "        time_col = 'Timestep'\n",
    "        timesteps = [str(i) for i in range(0,90)]\n",
    "        graph_cols, max_edges = [0,1,3,5], 100\n",
    "    elif input_data == 'petrochemicals':\n",
    "        divider = 'COUNTRY/TERRITORY'\n",
    "        inference_cols = ['PRODUCT', 'COUNTRY/TERRITORY']\n",
    "        time_col = 'Year'\n",
    "        timesteps = [str(i) for i in range(1978,2051)]\n",
    "        graph_cols, max_edges = [0,3], 15\n",
    "    elif input_data == 'unfccc':\n",
    "        divider='Party'\n",
    "        inference_cols = ['Party', 'Category']\n",
    "        time_col = 'Year'\n",
    "        timesteps = [str(i) for i in range(1990,2021)]\n",
    "        graph_cols, max_edges = [0], 100\n",
    "\n",
    "    ## Parameters\n",
    "    # max_test_set = 100000\n",
    "    # random_state = 2\n",
    "    # test_size = 0.3\n",
    "    regression = False\n",
    "\n",
    "    # Gap level\n",
    "    for gap_filling_level in [1,2,3]:\n",
    "        print('Gap level '+str(gap_filling_level))\n",
    "        X_train_pt = torch.load(data_path+'X_train_pt-'+input_data+'-'+str(gap_filling_level)+'.pt')#.cuda()\n",
    "        X_test_pt = torch.load(data_path+'X_test_pt-'+input_data+'-'+str(gap_filling_level)+'.pt')#.cuda()\n",
    "        y_train_pt = torch.load(data_path+'y_train_pt-'+input_data+'-'+str(gap_filling_level)+'.pt')#.cuda()\n",
    "        y_test_pt = torch.load(data_path+'y_test_pt-'+input_data+'-'+str(gap_filling_level)+'.pt')#.cuda()\n",
    "\n",
    "        X_train_unscaled = np.load(data_path+'X_train_unscaled-'+input_data+'-'+str(gap_filling_level)+'.npy')\n",
    "        X_test_unscaled = np.load(data_path+'X_test_unscaled-'+input_data+'-'+str(gap_filling_level)+'.npy')\n",
    "\n",
    "        if balance:\n",
    "          X_train_pt, X_test_pt, y_train_pt, y_test_pt, X_train_unscaled, X_test_unscaled = balance_classes_pt(X_train_pt, X_test_pt, y_train_pt, y_test_pt, X_train_unscaled=X_train_unscaled, X_test_unscaled=X_test_unscaled)\n",
    "\n",
    "        # Define graph\n",
    "        graph = Data()\n",
    "        graph.x = torch.cat((X_train_pt, X_test_pt))\n",
    "        graph.y = torch.cat((y_train_pt, y_test_pt))\n",
    "\n",
    "        # Train/test division\n",
    "        graph.train_mask = torch.tensor([True]*len(X_train_pt)+[False]*len(X_test_pt))\n",
    "        graph.test_mask = ~graph.train_mask\n",
    "\n",
    "        # Edge creation\n",
    "        input_df = pd.DataFrame(np.concatenate((X_train_unscaled[:,graph_cols].astype(int), X_test_unscaled[:,graph_cols].astype(int))))\n",
    "        graph.edge_index = pd_to_adj_matrix(input_df, columns=list(range(len(graph_cols))), max_edges=max_edges)\n",
    "\n",
    "        num_features = graph.x.shape[1]\n",
    "        num_classes = 4\n",
    "\n",
    "        for hidden_dim in hidden_sizes:\n",
    "            print(hidden_dim)\n",
    "            if input_data=='CT_manufacturing' and gap_filling_level==1:\n",
    "                models = [\n",
    "                   #('GCN', GCN(num_features=num_features, hidden_dim=hidden_dim, num_classes=num_classes)),#.cuda()),\n",
    "                    # ('SAGE', GraphSAGE(num_features=num_features, hidden_dim=hidden_dim, num_classes=num_classes)),#.cuda()),\n",
    "                    # ('GIN', GIN(num_features=num_features, hidden_dim=hidden_dim, num_classes=num_classes)),#.cuda()),\n",
    "                    # ('GAT', GAT(num_features=num_features, hidden_dim=hidden_dim, num_classes=num_classes)),\n",
    "                    ('GCLSTM', GC_LSTM(num_features=num_features, hidden_dim=hidden_dim, num_classes=num_classes)),#.cuda()),\n",
    "                    ('GUNET', GraphUNet(num_features=num_features, hidden_dim=hidden_dim, num_classes=num_classes))#.cuda())\n",
    "                    ]\n",
    "            else:\n",
    "                models = [\n",
    "                   #('GCN', GCN(num_features=num_features, hidden_dim=hidden_dim, num_classes=num_classes)),#.cuda()),\n",
    "                    ('SAGE', GraphSAGE(num_features=num_features, hidden_dim=hidden_dim, num_classes=num_classes)),#.cuda()),\n",
    "                    ('GIN', GIN(num_features=num_features, hidden_dim=hidden_dim, num_classes=num_classes)),#.cuda()),\n",
    "                    # ('GAT', GAT(num_features=num_features, hidden_dim=hidden_dim, num_classes=num_classes)),\n",
    "                    ('GCLSTM', GC_LSTM(num_features=num_features, hidden_dim=hidden_dim, num_classes=num_classes)),#.cuda()),\n",
    "                    ('GUNET', GraphUNet(num_features=num_features, hidden_dim=hidden_dim, num_classes=num_classes))#.cuda())\n",
    "                        ]\n",
    "            # # Define a list of models to train\n",
    "\n",
    "            # best_accuracy = {}\n",
    "            # best_learning_rate = {}\n",
    "            # best_hidden_size = {}\n",
    "\n",
    "            for model_name, model in models:\n",
    "\n",
    "                # best_accuracy[model_name] = 0\n",
    "                # best_learning_rate[model_name] = 0\n",
    "                # best_hidden_size[model_name] = 0\n",
    "\n",
    "                print(f\"Training {model_name}\")\n",
    "                for learning_rate in learning_rates:\n",
    "                    print(learning_rate)\n",
    "\n",
    "                    model_type = 'reg' if regression else 'clf'\n",
    "                    model_file = model_path + model_type + '_' + model_name + '_l' + str(str(gap_filling_level)) + '_' + date.today().strftime(\"%y%m%d\")\n",
    "\n",
    "                    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "                    criterion = nn.CrossEntropyLoss()\n",
    "                    epochs = 50\n",
    "\n",
    "                    loss, model, optimizer = train_graph(model, graph, optimizer, criterion, epochs=epochs)\n",
    "\n",
    "                    torch.save(model.state_dict(), model_file+'.pt')\n",
    "                    #\n",
    "                    if regression:\n",
    "                        y_pred = test_graph(model, graph)\n",
    "                    else:\n",
    "                        _, y_pred = test_graph(model, graph)\n",
    "\n",
    "                    scores = metrics(graph.y[graph.test_mask], y_pred, model_type)\n",
    "\n",
    "                    np.save(model_file+'_'+str(learning_rate)+'_'+str(hidden_size)+'_20iter.npy', scores)\n",
    "\n",
    "                    # # Check if the current combination of hyperparameters is the best\n",
    "                    # if scores['average_acc'] > best_accuracy[model_name]:\n",
    "                    #     best_accuracy[model_name] = scores['average_acc']\n",
    "                    #     best_learning_rate[model_name] = learning_rate\n",
    "                    #     best_hidden_size[model_name] = hidden_size\n",
    "\n",
    "        # Train the best models with the best hyperparameters for 100 epochs\n",
    "        # for model_name, model in models:\n",
    "        #     best_lr = best_learning_rate[model_name]\n",
    "        #     best_hs = best_hidden_size[model_name]\n",
    "        #     model_type = 'reg' if regression else 'clf'\n",
    "        #     model_file = model_path + model_type + '_' + model_name + '_l' + str(str(gap_filling_level)) + '_' + date.today().strftime(\"%y%m%d\")\n",
    "        #\n",
    "        #     optimizer = torch.optim.Adam(model.parameters(), lr=best_lr)\n",
    "        #     model.hidden_dim = best_hs\n",
    "        #     criterion = nn.CrossEntropyLoss()\n",
    "        #     epochs = 100\n",
    "        #\n",
    "        #     criterion = torch.nn.CrossEntropyLoss()\n",
    "        #\n",
    "        #     loss, model, optimizer = train_graph(model, graph, optimizer, criterion, epochs=epochs)\n",
    "        #\n",
    "        #     torch.save(model.state_dict(), model_file+'.pt')\n",
    "        #\n",
    "        #     if regression:\n",
    "        #         y_pred = test_graph(model, graph)\n",
    "        #     else:\n",
    "        #         _, y_pred = test_graph(model, graph)\n",
    "        #\n",
    "        #     scores = metrics(graph.y[graph.test_mask], y_pred, model_type)\n",
    "        #\n",
    "        #     np.save(model_file+'_'+str(best_lr)+'_'+str(best_hs)+'_100iter.npy', scores)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "{'confusion': array([[ 7972,  5030,  5578,  2328],\n        [ 2878,  9261,  7109,  1660],\n        [ 1206,  5932, 11457,  2313],\n        [  829,  1949,  6823, 11307]], dtype=int64),\n 'overall_acc': 0.47824995217141764,\n 'average_acc': 0.47824995217141764,\n 'kappa': 0.3043332695618902,\n 'IoU': 0.32040123437967016}"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, GINConv, GATConv\n",
    "\n",
    "# Graph Convolutional Network (GCN)\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# class GCN(nn.Module):\n",
    "#     def __init__(self, num_features, hidden_dim, num_classes):\n",
    "#         super(GCN, self).__init__()\n",
    "#         self.conv1 = GCNConv(num_features, hidden_dim)\n",
    "#         self.conv2 = GCNConv(hidden_dim, num_classes)\n",
    "#\n",
    "#     def forward(self, data):\n",
    "#         x, edge_index = data.x, data.edge_index\n",
    "#         x = F.relu(self.conv1(x, edge_index))\n",
    "#         x = F.dropout(x, training=self.training)\n",
    "#         x = self.conv2(x, edge_index)\n",
    "#         return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "#GraphSAGE\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim, num_classes):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(num_features, hidden_dim)\n",
    "        self.conv2 = SAGEConv(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "# Graph Isomorphism Network (GIN)\n",
    "class GIN(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim, num_classes):\n",
    "        super(GIN, self).__init__()\n",
    "        self.conv1 = GINConv(torch.nn.Sequential(torch.nn.Linear(num_features, hidden_dim), torch.nn.ReLU(),\n",
    "                                                 torch.nn.Linear(hidden_dim, hidden_dim)))\n",
    "        self.conv2 = GINConv(torch.nn.Sequential(torch.nn.Linear(hidden_dim, hidden_dim), torch.nn.ReLU(),\n",
    "                                                 torch.nn.Linear(hidden_dim, num_classes)))\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "# Graph Attention Network (GAT)\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim, num_classes):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(num_features, hidden_dim, heads=8, dropout=0.6)\n",
    "        self.conv2 = GATConv(hidden_dim * 8, num_classes, heads=1, concat=False, dropout=0.6)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "# Graph Convolutional LSTM (GC-LSTM)\n",
    "class GC_LSTM(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim, num_classes):\n",
    "        super(GC_LSTM, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_dim)\n",
    "        self.lstm = torch.nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.lin = torch.nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = x.unsqueeze(0)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x.squeeze(0)\n",
    "        x = self.lin(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "# Graph U-Net\n",
    "class GraphUNet(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim, num_classes):\n",
    "        super(GraphUNet, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x1 = self.conv1(x, edge_index)\n",
    "        x1 = F.relu(x1)\n",
    "        x2 = self.conv2(x1, edge_index)\n",
    "        x2 = F.relu(x2)\n",
    "        x3 = self.conv3(x2, edge_index)\n",
    "        return F.log_softmax(x3, dim=1)\n",
    "\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "# class GCN(nn.Module):\n",
    "#     def __init__(self, num_features, hidden_dim, num_classes):\n",
    "#         super(GCN, self).__init__()\n",
    "#         self.conv1 = GCNConv(num_features, hidden_dim)\n",
    "#         self.conv2 = GCNConv(hidden_dim, num_classes)\n",
    "#\n",
    "#     def forward(self, data):\n",
    "#         x, edge_index = data.x, data.edge_index\n",
    "#         x = F.relu(self.conv1(x, edge_index))\n",
    "#         x = F.dropout(x, training=self.training)\n",
    "#         x = self.conv2(x, edge_index)\n",
    "#         return F.log_softmax(x, dim=1)\n",
    "\n",
    "from torch_geometric.nn import SAGEConv\n",
    "#\n",
    "class SAGE(nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim, num_classes):\n",
    "        super(SAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(num_features, hidden_dim)\n",
    "        self.conv2 = SAGEConv(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "# Create a random graph dataset with 100000 nodes and 9 features\n",
    "# x_train = torch.randn(100000, 9)\n",
    "# edge_index_train = torch.randint(0, 100000, (2, 100000))\n",
    "#\n",
    "# data = Data(x=x_train, edge_index=edge_index_train)\n",
    "#\n",
    "# # Instantiate the graph neural network models\n",
    "# gcn_model = GCN(num_features=9, hidden_dim=64, num_classes=10)\n",
    "# graphsage_model = GraphSAGE(num_features=9, hidden_dim=64, num_classes=10)\n",
    "# gin_model = GIN(num_features=9, hidden_dim=64, num_classes=10)\n",
    "# gat_model = GAT(num_features=9, hidden_dim=64, num_classes=10)\n",
    "# gclstm_model = GC_LSTM(num_features=9, hidden_dim=64, num_classes=10)\n",
    "# graphunet_model = GraphUNet(num_features=9, hidden_dim=64, num_classes=10)\n",
    "#\n",
    "# # Perform forward pass\n",
    "# output_gcn = gcn_model(data)\n",
    "# output_graphsage = graphsage_model(data)\n",
    "# output_gin = gin_model(data)\n",
    "# output_gat = gat_model(data)\n",
    "# output_gclstm = gclstm_model(data)\n",
    "# output_graphunet = graphunet_model(data)\n",
    "#\n",
    "# # Print the output shapes\n",
    "# print(\"GCN output shape:\", output_gcn.shape)\n",
    "# print(\"GraphSAGE output shape:\", output_graphsage.shape)\n",
    "# print(\"GIN output shape:\", output_gin.shape)\n",
    "# print(\"GAT output shape:\", output_gat.shape)\n",
    "# print(\"GC-LSTM output shape:\", output_gclstm.shape)\n",
    "# print(\"Graph U-Net output shape:\", output_graphunet.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "## Define torch geometric functions and models\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "# class GCN(nn.Module):\n",
    "#     def __init__(self, num_features, hidden_dim, num_classes):\n",
    "#         super(GCN, self).__init__()\n",
    "#         self.conv1 = GCNConv(num_features, hidden_dim)\n",
    "#         self.conv2 = GCNConv(hidden_dim, num_classes)\n",
    "#\n",
    "#     def forward(self, data):\n",
    "#         x, edge_index = data.x, data.edge_index\n",
    "#         x = F.relu(self.conv1(x, edge_index))\n",
    "#         x = F.dropout(x, training=self.training)\n",
    "#         x = self.conv2(x, edge_index)\n",
    "#         return F.log_softmax(x, dim=1)\n",
    "\n",
    "# from torch_geometric.nn import SAGEConv\n",
    "#\n",
    "# class SAGE(nn.Module):\n",
    "#     def __init__(self, num_features, hidden_dim, num_classes):\n",
    "#         super(SAGE, self).__init__()\n",
    "#         self.conv1 = SAGEConv(num_features, hidden_dim)\n",
    "#         self.conv2 = SAGEConv(hidden_dim, num_classes)\n",
    "#\n",
    "#     def forward(self, data):\n",
    "#         x, edge_index = data.x, data.edge_index\n",
    "#         x = F.relu(self.conv1(x, edge_index))\n",
    "#         x = F.dropout(x, training=self.training)\n",
    "#         x = self.conv2(x, edge_index)\n",
    "#         return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "def train_graph(model, data, optimizer, criterion=torch.nn.CrossEntropyLoss(), epochs=100, verbose=True):\n",
    "    \"\"\"Training function for pytorch models\"\"\"\n",
    "    print('in')\n",
    "    model.train()\n",
    "    print('Train activated')\n",
    "    for epoch in range(epochs):\n",
    "        print(str(epoch))\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "        print('zeroed')\n",
    "        out = model(data) # Forward pass\n",
    "        print('out')\n",
    "        loss = criterion(out[data.train_mask], data.y[data.train_mask]) # Compute loss\n",
    "        print('loss')\n",
    "        if verbose:\n",
    "            print('Epoch {}: train loss: {}'.format(epoch, loss.item()))\n",
    "        loss.backward()  # Derive gradients\n",
    "        optimizer.step()  # Update parameters based on gradients\n",
    "\n",
    "    return loss, model, optimizer\n",
    "\n",
    "def test_graph(model, data, categorical=True):\n",
    "    \"\"\"Test function for pytorch models\"\"\"\n",
    "    model.eval()\n",
    "    pred = model(data) # Forward pass\n",
    "    y_pred = pred[data.test_mask]\n",
    "\n",
    "    if categorical: # Get category with the highest probability\n",
    "        _, y_pred_cats = torch.max(y_pred, dim = 1)\n",
    "        return y_pred, y_pred_cats\n",
    "    else: # Return raw prediction\n",
    "        return y_pred"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "_hwBtufBzFhf"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from torch_geometric.utils import remove_self_loops\n",
    "from scipy.sparse import coo_matrix, diags\n",
    "\n",
    "def pd_to_adj_matrix(df:pd.DataFrame, columns:list, weights:list = False, remove_self_conns:bool = True, max_edges = 100, verbose:bool = True):\n",
    "    \"\"\"Form pytorch COO format adjacency matrix from pandas dataframe columns\"\"\"\n",
    "    groups = [group.index.values.astype(int) for col in columns for _, group in df.groupby(col)]\n",
    "\n",
    "    rows = torch.tensor(np.concatenate([np.tile(g.flatten(), min(len(g), max_edges)) for g in groups]), dtype=torch.long)\n",
    "    cols = torch.tensor(np.concatenate([np.repeat(g.flatten(), min(len(g), max_edges)) for g in groups]), dtype=torch.long)\n",
    "\n",
    "    if weights is False:\n",
    "        weight_vector = torch.tensor(np.ones(len(rows), dtype=int), dtype=torch.float)\n",
    "    else: weight_vector = torch.tensor(np.ones(len(rows), dtype=int)*weights, dtype=torch.long)\n",
    "\n",
    "    adjacency = torch_sparse.SparseTensor(row=rows, col=cols, value=weight_vector)\n",
    "        #coo_matrix((rows, cols), weight_vector, size=(len(df), len(df)))\n",
    "\n",
    "    #if remove_self_conns:\n",
    "        #adjacency = remove_self_loops(adjacency)\n",
    "        #adjacency -= diags(adjacency.diagonal(k=0))\n",
    "\n",
    "    #if verbose:\n",
    "        # print(f\"Size of adjacency in memory: {(adjacency.data.nbytes + adjacency.indptr.nbytes + adjacency.indices.nbytes) / 8 / 1024 ** 3:.3f}GB\")\n",
    "        #print(f\"Numb edges: {len(adjacency.indices)}\")\n",
    "    return adjacency\n",
    "\n",
    "# ## Attempts for full edges (3 billion) - other solution, kneighbors graph, faiss\n",
    "# for group in tqdm(groups[10:20]):\n",
    "#     edges += list(product(group, repeat=2))\n",
    "#\n",
    "#     all_pairs = pd.DataFrame()\n",
    "#\n",
    "#\n",
    "# x_arr = np.array(graph.x[:,0]).reshape(-1,1)\n",
    "# unique_vals = np.unique(x_arr)\n",
    "# for u_val in tqdm(unique_vals):\n",
    "#     locs = locate(x_arr, lambda x: x==u_val)\n",
    "#     df = pd.DataFrame(locs)\n",
    "#     vals = df.values.astype(int)\n",
    "#     pairs = product(vals,vals)\n",
    "#     pair_df = pd.DataFrame(pairs).astype(int)\n",
    "#     all_pairs = pd.concat((all_pairs, pair_df))"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "AniTWW33zFhf"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## torch geometric classifier training - Graph definition"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "2jXmVsonzFhg"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 1.5003718137741089\n",
      "Epoch 1: train loss: 1.3746167421340942\n",
      "Epoch 2: train loss: 1.2647819519042969\n",
      "Epoch 3: train loss: 1.1701548099517822\n",
      "Epoch 4: train loss: 1.0908373594284058\n",
      "Epoch 5: train loss: 1.0272905826568604\n",
      "Epoch 6: train loss: 0.9780631065368652\n",
      "Epoch 7: train loss: 0.942402184009552\n",
      "Epoch 8: train loss: 0.9171971678733826\n",
      "Epoch 9: train loss: 0.8992316722869873\n",
      "Epoch 0: train loss: 1.356438398361206\n",
      "Epoch 1: train loss: 1.1591078042984009\n",
      "Epoch 2: train loss: 1.067864179611206\n",
      "Epoch 3: train loss: 1.0133190155029297\n",
      "Epoch 4: train loss: 0.9641536474227905\n",
      "Epoch 5: train loss: 0.9199181795120239\n",
      "Epoch 6: train loss: 0.885001540184021\n",
      "Epoch 7: train loss: 0.8621838688850403\n",
      "Epoch 8: train loss: 0.847547173500061\n",
      "Epoch 9: train loss: 0.8411931991577148\n"
     ]
    }
   ],
   "source": [
    "## Loop over names and model definitions\n",
    "\n",
    "regression = False\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = 4\n",
    "hidden_dim = 64\n",
    "\n",
    "names = ['GCN-h'+str(hidden_dim), 'SAGE-h'+str(hidden_dim)] #['GCN-h'+str(hidden_dim), 'GAT-h'+str(hidden_dim), 'SAGE-h'+str(hidden_dim), 'GIN-h'+str(hidden_dim)]\n",
    "models = [GCN(input_dim, hidden_dim, output_dim), SAGE(input_dim, hidden_dim, output_dim)] #[GCN(input_dim, hidden_dim, output_dim), GAT(input_dim, hidden_dim, output_dim), SAGE(input_dim, hidden_dim, output_dim), GIN(input_dim, hidden_dim, output_dim)]\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for model_name, model in zip(names, models):\n",
    "    model_type = 'reg' if regression else 'clf'\n",
    "    model_file = model_path+model_type+'_'+model_name+'_l'+str(gap_filling_level)+'_'+date.today().strftime(\"%y%m%d\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    loss, model, optimizer = train_graph(model, graph, optimizer, criterion, epochs=10)\n",
    "\n",
    "    torch.save(model.state_dict(), model_file+'.pt')\n",
    "\n",
    "    if regression:\n",
    "        y_pred = test_graph(model, graph)\n",
    "    else:\n",
    "        _, y_pred = test_graph(model, graph)\n",
    "\n",
    "    scores = metrics(y_test_pt, y_pred, model_type)\n",
    "\n",
    "    np.save(model_file+'.npy', scores)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "ztpBHYxZzFhg",
    "outputId": "5297fc2a-9c50-42ad-e7dd-fac63d225bf1"
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}