{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## This notebook takes processed data for the 3 datsets and creates machine learning ready datasets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "import sklearn.linear_model\n",
    "\n",
    "\"\"\"Define graph forming functions\"\"\"\n",
    "# Import packages\n",
    "\n",
    "import joblib\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "\n",
    "from sklearn import tree\n",
    "import sklearn.svm as svm\n",
    "from sklearn import ensemble\n",
    "import sklearn.metrics as skm\n",
    "import sklearn.preprocessing as pp\n",
    "import sklearn.linear_model as lms\n",
    "import sklearn.neural_network as skl_nn\n",
    "import sklearn.neighbors as neighbors\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import sklearn.model_selection as model_sel\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch_sparse\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Input data\n",
    "random_state = 2\n",
    "np.random.seed(random_state)\n",
    "data_path = \"..\\data\\classification_inputs/\"\n",
    "\n",
    "max_test_set = 100000"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def filter_for_start_yr(df, start_col, end_col) -> pd.DataFrame:\n",
    "    \"\"\"Convert dataframe of plants with entry for each year into dataframe with row for each year\"\"\"\n",
    "    # Get rid of emissions for years before start year\n",
    "    df['Age'] = df['Year'].astype(int) - df[start_col].astype(int)\n",
    "    df = df[df['Age'] >= 0]\n",
    "\n",
    "    # Get rid of emissions for years after end years\n",
    "    df['ToGo'] = df[end_col].astype(int) - df['Year'].astype(int)\n",
    "    df = df[df['ToGo'] >= 0]\n",
    "\n",
    "    df[['START_YR', 'END_YR']] = df[['START_YR', 'END_YR']].astype(int)\n",
    "\n",
    "    return df.drop(columns=['ToGo'])\n",
    "\n",
    "def pivot_data_dfs(df:pd.DataFrame, time_col) -> [pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Pivot data dataframes to get each entry and all year/month values in rows\"\"\"\n",
    "    feature_cols = [i for i in df.columns if i not in [time_col, 'Emissions']]\n",
    "    df_pivoted = df.pivot(index=feature_cols,\n",
    "                          columns=time_col,\n",
    "                          values = 'Emissions').reset_index()\n",
    "    return df_pivoted, feature_cols\n",
    "\n",
    "\n",
    "def melt_data_dfs(df:pd.DataFrame, feature_cols, time_col) -> pd.DataFrame:\n",
    "    \"\"\"Melt data dataframes to get row per date entry for each facility\"\"\"\n",
    "    melted = df.melt(id_vars=feature_cols, var_name=time_col, value_name='Emissions').dropna(subset=['Emissions'])\n",
    "    melted[time_col] = melted[time_col].astype(int)\n",
    "    \n",
    "    if 'START_YR' in df.columns and 'END_YR' in df.columns:\n",
    "        melted = filter_for_start_yr(melted, 'START_YR', 'END_YR')\n",
    "\n",
    "    return melted\n",
    "\n",
    "## Convert train and test sets into ML ready sets\n",
    "def series_to_bins(series:pd.Series, bins:list=None, labels:list=None, positive:bool=True):\n",
    "    # Convert a continuous pandas dataframe column into discrete bins\n",
    "    if bins is None:\n",
    "        bin_series = series[series!=0] if positive else series\n",
    "        bins = [min(bin_series.min(),0)-0.01, bin_series.quantile(0.25), bin_series.quantile(0.5), bin_series.quantile(0.75), bin_series.max()+0.01]\n",
    "    if labels is None: labels = list(range(len(bins)-1))\n",
    "\n",
    "    transformer = pp.FunctionTransformer(\n",
    "        pd.cut, kw_args={'bins': bins, 'labels': labels, 'retbins': False}\n",
    "    )\n",
    "    return bins, transformer.fit_transform(series)\n",
    "\n",
    "\n",
    "def preprocess_yearly(train_set, test_set, y_col='Emissions'):\n",
    "    \"\"\"Digitise train and test sets\"\"\"\n",
    "\n",
    "    # Create Y\n",
    "    bins, y_train_clf = series_to_bins(train_set[y_col])\n",
    "    _, y_test_clf = series_to_bins(test_set[y_col], bins=[test_set[y_col].min()-0.01]+bins[1:-1]+[test_set[y_col].max()+0.01])\n",
    "\n",
    "    y_train_reg, y_test_reg = train_set[y_col], test_set[y_col]\n",
    "    train_set, test_set = train_set.drop(columns=[y_col]), test_set.drop(columns=[y_col])\n",
    "\n",
    "    # Create X\n",
    "    # Deal with string columns\n",
    "    x_enc = pp.OrdinalEncoder()\n",
    "    string_cols = list(train_set.select_dtypes(include='object').columns)\n",
    "    train_set[string_cols] = train_set[string_cols].astype(str)\n",
    "    test_set[string_cols] = test_set[string_cols].astype(str)\n",
    "    x_strings = pd.concat((train_set[string_cols], test_set[string_cols]))\n",
    "    x_enc.fit(x_strings)\n",
    "\n",
    "    # Make float columns into int columns\n",
    "    float_cols = list(train_set.select_dtypes(include='float').columns)\n",
    "    train_set[float_cols], test_set[float_cols] = train_set[float_cols].astype(int), test_set[float_cols].astype(int)\n",
    "\n",
    "    if 'LATITUDE' in list(train_set.columns) and 'LONGITUDE' in list(train_set.columns):\n",
    "        train_set[['LATITUDE', 'LONGITUDE']] = (train_set[['LATITUDE', 'LONGITUDE']].astype(int)+[90, 180])\n",
    "        test_set[['LATITUDE', 'LONGITUDE']] = (test_set[['LATITUDE', 'LONGITUDE']].astype(int)+[90, 180])\n",
    "\n",
    "    int_cols = list(train_set.select_dtypes(include='integer').columns)\n",
    "    x_ints_min = pd.concat((train_set[int_cols], test_set[int_cols])).min().values\n",
    "    x_ints_train = train_set[int_cols] - x_ints_min\n",
    "    x_ints_test = test_set[int_cols] - x_ints_min\n",
    "\n",
    "    X_train = np.concatenate((x_enc.transform(train_set[string_cols]),\n",
    "                              x_ints_train.values), axis=1)\n",
    "    X_test = np.concatenate((x_enc.transform(test_set[string_cols]),\n",
    "                              x_ints_test.values), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    return X_train, X_test, y_train_clf, y_test_clf, y_train_reg, y_test_reg, x_enc\n",
    "\n",
    "def save_decoded_X(X, x_enc, cols, used, name):\n",
    "    min_years = [used['START_YR'].astype(int).min(), 1978]\n",
    "    X_inv = np.concatenate((x_enc.inverse_transform(X[:,:-4]), (X[:,-4:-2]+min_years).astype(int), X[:,-2:]), axis=1)\n",
    "    pd.DataFrame(X_inv, columns=list(columns[:-2]+['Year']+columns[-2:])).to_csv(name+'.csv')\n",
    "\n",
    "# Function to split rows into two DataFrames\n",
    "def split_rows(group, test_fraction):\n",
    "\n",
    "    num_rows = group.shape[0]\n",
    "    if num_rows == 1:\n",
    "        return None, None  # Exclude groups with only one sample\n",
    "\n",
    "    num_sampled_rows = int(min(max(test_fraction * num_rows, 1), num_rows-1))  # At least one sample for each group\n",
    "\n",
    "    test_df = group.sample(n=num_sampled_rows)\n",
    "    train_df = group.drop(test_df.index)\n",
    "\n",
    "    return train_df, test_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def create_yearly_data(data, input_data, gap_filling_level, timesteps, test_size):\n",
    "\n",
    "    if gap_filling_level == 1:\n",
    "        \"\"\"Gap filling level 1\n",
    "        Divide training data points (year for particular facility) from test data points -> Unknown year fill\"\"\"\n",
    "\n",
    "        train_set, test_set = data.copy(), data.copy()\n",
    "\n",
    "        # Calculate the number of entries to mask in each row\n",
    "        num_entries = data[timesteps].shape[1]\n",
    "        num_masked_entries = int(test_size * num_entries)\n",
    "\n",
    "        # Mask and unmask entries in each row\n",
    "        mask = np.zeros(data[timesteps].shape, dtype=bool)\n",
    "        for i in range(data[timesteps].shape[0]):\n",
    "            indices = np.random.choice(num_entries, size=num_masked_entries, replace=False)\n",
    "            mask[i, indices] = True\n",
    "\n",
    "        train_set[timesteps] = train_set[timesteps].mask(mask)\n",
    "        test_set[timesteps] = test_set[timesteps].mask(~mask)\n",
    "\n",
    "        feature_cols = [i for i in train_set.columns if i not in timesteps]\n",
    "        train_yearly, test_yearly = melt_data_dfs(train_set, feature_cols, time_col), melt_data_dfs(test_set, feature_cols, time_col)\n",
    "\n",
    "    elif gap_filling_level == 2:\n",
    "        \"\"\"Gap filling level 2\n",
    "        Divide training plants from test plants -> Unknown plant fill\"\"\"\n",
    "\n",
    "        grouping_cols = inference_cols if len(inference_cols) > 2 else [inference_cols[0]]\n",
    "\n",
    "        split_rows_partial = partial(split_rows, test_fraction=test_size)\n",
    "        grouped_df = data.groupby(grouping_cols)\n",
    "\n",
    "        train_dfs, test_dfs = zip(*grouped_df.apply(split_rows_partial))\n",
    "        train_set, test_set = pd.concat(train_dfs), pd.concat(test_dfs)\n",
    "\n",
    "        feature_cols = [i for i in train_set.columns if i not in timesteps]\n",
    "        train_yearly, test_yearly = melt_data_dfs(train_set, feature_cols, time_col), melt_data_dfs(test_set, feature_cols, time_col)\n",
    "\n",
    "    elif gap_filling_level == 3:\n",
    "        \"\"\"Gap filling level 3\n",
    "        Divide training countries or products from test countries or products -> Unknown country/product fill\"\"\"\n",
    "        gap3cols = [i for i in inference_cols if i!=divider]\n",
    "\n",
    "        split_rows_partial = partial(split_rows, test_fraction=test_size)\n",
    "        grouped_df = data.groupby(gap3cols)\n",
    "\n",
    "        train_dfs, test_dfs = zip(*grouped_df.apply(split_rows_partial))\n",
    "        train_set, test_set = pd.concat(train_dfs), pd.concat(test_dfs)\n",
    "        # train_set = data.groupby(gap3cols).apply(lambda x: x.sample(frac=1-test_size)).reset_index(drop=True)\n",
    "        # test_set = data.groupby(gap3cols).apply(lambda x: x.sample(frac=test_size)).reset_index(drop=True)\n",
    "\n",
    "        # train_divs, test_divs = model_sel.train_test_split(data[divider].unique(), test_size=0.3, random_state=1)\n",
    "        # train_set, test_set = [data[[i in divs for i in data[divider]]] for divs in [train_divs, test_divs]]\n",
    "\n",
    "        feature_cols = [i for i in train_set.columns if i not in timesteps]\n",
    "        train_yearly, test_yearly = melt_data_dfs(train_set, feature_cols, time_col), melt_data_dfs(test_set, feature_cols, time_col)\n",
    "\n",
    "    else: print('Please choose a gap filling level.')\n",
    "\n",
    "    return feature_cols, train_yearly, test_yearly"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def custom_interpolate(row):\n",
    "    if row.count() >= 3:  # Check if there are enough values for polynomial interpolation\n",
    "        return row.interpolate(method='polynomial', order=order, limit_direction='both')\n",
    "    else:\n",
    "        return row.interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "def metrics(y_true, y_pred, model_type='clf'):\n",
    "    if model_type == 'clf':\n",
    "        metric_dict = {'confusion': skm.confusion_matrix(y_true, y_pred),\n",
    "                       'overall_acc': skm.accuracy_score(y_true, y_pred),\n",
    "                       'average_acc': skm.balanced_accuracy_score(y_true, y_pred),\n",
    "                       'kappa': skm.cohen_kappa_score(y_true, y_pred),\n",
    "                       'IoU': skm.jaccard_score(y_true, y_pred, average='weighted')}\n",
    "    elif model_type == 'reg':\n",
    "        metric_dict = {'r2': skm.r2_score(y_true, y_pred),\n",
    "                       'mae': skm.mean_absolute_error(y_true, y_pred),\n",
    "                       'mse': skm.mean_squared_error(y_true, y_pred)}\n",
    "\n",
    "    else: raise 'Incorrect model type'\n",
    "\n",
    "    return metric_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def gap_interpolation(train_yearly:pd.DataFrame, test_yearly:pd.DataFrame, years:list, feature_cols:list, inference_cols:list, gap_filling_level:int, interpolation:str='linear', order:int=None, divider:str=None):\n",
    "    # Interpolate missing values in data rows of training set\n",
    "    df_train = train_yearly.pivot(index=feature_cols, columns=time_col, values='Emissions').reset_index()\n",
    "\n",
    "    if gap_filling_level == 1:\n",
    "        if order is not None:\n",
    "            train_years = df_train[years]\n",
    "            train_years.columns = train_years.columns.astype(int)\n",
    "            # sufficient = train_years[train_years.count(axis=1) > order+1]\n",
    "            # insufficient = train_years[train_years.count(axis=1) <= order+1]\n",
    "            df_train[years] = train_years.transpose().apply(custom_interpolate).transpose()\n",
    "                #pd.concat((sufficient.interpolate(method='polynomial', order=order, axis=1, limit_direction='both'),\n",
    "                                       #  insufficient.interpolate(method='linear', axis=1, limit_direction='both'))).sort_index()\n",
    "\n",
    "        else: df_train[years] = df_train[years].interpolate(method=model, order=order, axis=1, limit_direction='both')\n",
    "        df_train[years] = df_train[years].interpolate(method='pad', axis=1).interpolate(method='bfill', axis=1)\n",
    "        pred_yearly = melt_data_dfs(df_train, feature_cols, time_col)\n",
    "        inference_cols = feature_cols\n",
    "\n",
    "    else:\n",
    "        if gap_filling_level == 3 or inference_cols == feature_cols:\n",
    "            inference_cols = [item for item in inference_cols if item != divider]\n",
    "        # interp = train_yearly[inference_cols+years].groupby(inference_cols).mean()\n",
    "        #\n",
    "        # pred_yearly = pd.melt(interp.reset_index(), id_vars=inference_cols, value_vars=years, var_name=time_col, value_name='Emissions')\n",
    "        pred_yearly = train_yearly.groupby(inference_cols+[time_col]).mean()['Emissions'].reset_index()\n",
    "\n",
    "    test_cleared = test_yearly.dropna(subset=['Emissions'])\n",
    "    if len(test_cleared) > max_test_set:\n",
    "        test_cleared = test_cleared.sample(n=max_test_set, random_state=random_state)\n",
    "    preds_merged = test_cleared.merge(pred_yearly, how='left', on=inference_cols+[time_col]).dropna(subset=['Emissions_y'])\n",
    "    # test_cleared = test_yearly.dropna(subset=['Emissions'])\n",
    "    # if len(test_cleared) > max_test_set:\n",
    "    #     test_cleared = test_cleared.sample(n=max_test_set, random_state=random_state)\n",
    "    # preds_merged = test_cleared.merge(pred_yearly, how='left', on=inference_cols+[time_col]).dropna(subset=['Emissions_y'])\n",
    "\n",
    "    return preds_merged['Emissions_x'], preds_merged['Emissions_y']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from torch_geometric.utils import remove_self_loops\n",
    "from scipy.sparse import coo_matrix, diags\n",
    "\n",
    "def pd_to_adj_matrix(df:pd.DataFrame, columns:list, weights:list = False, remove_self_conns:bool = True, max_edges = 100, verbose:bool = True):\n",
    "    \"\"\"Form pytorch COO format adjacency matrix from pandas dataframe columns\"\"\"\n",
    "    groups = [group.index.values.astype(int) for col in columns for _, group in df.groupby(col)]\n",
    "\n",
    "    rows = torch.tensor(np.concatenate([np.tile(g.flatten(), min(len(g), max_edges)) for g in groups]), dtype=torch.long)\n",
    "    cols = torch.tensor(np.concatenate([np.repeat(g.flatten(), min(len(g), max_edges)) for g in groups]), dtype=torch.long)\n",
    "\n",
    "    if weights is False:\n",
    "        weight_vector = torch.tensor(np.ones(len(rows), dtype=int), dtype=torch.float)\n",
    "    else: weight_vector = torch.tensor(np.ones(len(rows), dtype=int)*weights, dtype=torch.long)\n",
    "\n",
    "    adjacency = torch_sparse.SparseTensor(row=rows, col=cols, value=weight_vector)\n",
    "\n",
    "    return adjacency\n",
    "\n",
    "def balance_classes_pt(X_train, X_test, y_train, y_test, col_name = 'Emissions'):\n",
    "    y_train_pd = pd.Series(y_train, name=col_name)\n",
    "    min_count = y_train_pd.reset_index().groupby(col_name).count().min()\n",
    "    y_train_df = y_train_pd.reset_index().groupby(col_name).sample(min_count.values)\n",
    "    y_train = y_train_df[col_name].values\n",
    "    X_train = X_train[y_train_df.index]\n",
    "\n",
    "    y_test_pd = pd.Series(y_test, name=col_name)\n",
    "    min_count = y_test_pd.reset_index().groupby(col_name).count().min()\n",
    "    y_test_df = y_test_pd.reset_index().groupby(col_name).sample(min_count.values)\n",
    "    y_test = y_test_df[col_name].values\n",
    "    X_test = X_test[y_test_df.index]\n",
    "\n",
    "    return X_train, X_test, torch.tensor(y_train), torch.tensor(y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Inputs\n",
    "# Input data\n",
    "data_path = \"C:/Users\\lukec\\PycharmProjects\\emissions-tracking-conda\\emissions-tracking\\data\\classification_inputs/\"\n",
    "for input_data in ['CT_manufacturing','petrochemicals','unfccc']:\n",
    "    print(input_data)\n",
    "     # Output data\n",
    "    model_path = 'C:/Users\\lukec\\PycharmProjects\\emissions-tracking-conda\\emissions-tracking\\models/datasets/'\n",
    "\n",
    "    # Define divider for level 3\n",
    "    if input_data == 'CT_manufacturing':\n",
    "        divider = 'iso3_country'\n",
    "        inference_cols = ['iso3_country', 'original_inventory_sector', 'asset_type']\n",
    "        time_col = 'Timestep'\n",
    "        timesteps = [str(i) for i in range(0,90)]\n",
    "        graph_cols, max_edges = [0,1,3,5], 100\n",
    "    elif input_data == 'petrochemicals':\n",
    "        divider = 'COUNTRY/TERRITORY'\n",
    "        inference_cols = ['PRODUCT', 'COUNTRY/TERRITORY']\n",
    "        time_col = 'Year'\n",
    "        timesteps = [str(i) for i in range(1978,2051)]\n",
    "        graph_cols, max_edges = [0,3], 30\n",
    "    elif input_data == 'unfccc':\n",
    "        divider='Party'\n",
    "        inference_cols = ['Party', 'Category']\n",
    "        time_col = 'Year'\n",
    "        timesteps = [str(i) for i in range(1990,2021)]\n",
    "        graph_cols, max_edges = [0], 100\n",
    "\n",
    "    ## Parameters\n",
    "    max_test_set = 100000\n",
    "    random_state = 2\n",
    "    test_size = 0.3\n",
    "    regression = False\n",
    "\n",
    "    data = pd.read_csv(data_path+input_data+'.csv')\n",
    "    # Gap level\n",
    "    for gap_filling_level in [1,2,3]:\n",
    "        print('Gap level'+str(gap_filling_level))\n",
    "        feature_cols, train_yearly, test_yearly = create_yearly_data(data, input_data, gap_filling_level, timesteps, test_size)\n",
    "        # Digitise train & test sets\n",
    "        X_train_unscaled, X_test_unscaled, y_train_clf, y_test_clf, y_train_reg, y_test_reg, x_enc = preprocess_yearly(train_yearly, test_yearly)\n",
    "\n",
    "\n",
    "\n",
    "        # Scale datasets & save as .npy\n",
    "        scaler = pp.StandardScaler().fit(X_train_unscaled)\n",
    "        X_train, X_test = scaler.transform(X_train_unscaled), scaler.transform(X_test_unscaled)\n",
    "\n",
    "        # Create pytorch tensors\n",
    "        X_train_pt, X_test_pt, y_train_pt, y_test_pt = [torch.Tensor(array) for array in [X_train, X_test, y_train_clf.values, y_test_clf.values]]\n",
    "        y_train_pt, y_test_pt = [df.to(torch.long) for df in [y_train_pt, y_test_pt]]\n",
    "\n",
    "        pd.DataFrame(y_train_clf).to_csv(model_path+'y_train-'+input_data+'-'+str(gap_filling_level)+'.csv')\n",
    "        pd.DataFrame(y_test_clf).to_csv(model_path+'y_test-'+input_data+'-'+str(gap_filling_level)+'.csv')\n",
    "\n",
    "        # Scale datasets & save as .npy\n",
    "        scaler = pp.StandardScaler().fit(X_train_unscaled)\n",
    "        X_train, X_test = scaler.transform(X_train_unscaled), scaler.transform(X_test_unscaled)\n",
    "\n",
    "        np.save(model_path+'X_train_unscaled-'+input_data+'-'+str(gap_filling_level)+'.npy', X_train_unscaled)\n",
    "        np.save(model_path+'X_test_unscaled-'+input_data+'-'+str(gap_filling_level)+'.npy', X_test_unscaled)\n",
    "\n",
    "        np.save(model_path+'X_train-'+input_data+'-'+str(gap_filling_level)+'.npy', X_train)\n",
    "        np.save(model_path+'X_test-'+input_data+'-'+str(gap_filling_level)+'.npy', X_test)\n",
    "\n",
    "        # Create pytorch tensors\n",
    "        X_train_pt, X_test_pt, y_train_pt, y_test_pt = [torch.Tensor(array) for array in [X_train, X_test, y_train_clf.values, y_test_clf.values]]\n",
    "        y_train_pt, y_test_pt = [df.to(torch.long) for df in [y_train_pt, y_test_pt]]\n",
    "\n",
    "        torch.save(X_train_pt, model_path+'X_train_pt-'+input_data+'-'+str(gap_filling_level)+'.pt')\n",
    "        torch.save(X_test_pt, model_path+'X_test_pt-'+input_data+'-'+str(gap_filling_level)+'.pt')\n",
    "        torch.save(y_train_pt, model_path+'y_train_pt-'+input_data+'-'+str(gap_filling_level)+'.pt')\n",
    "        torch.save(y_test_pt, model_path+'y_test_pt-'+input_data+'-'+str(gap_filling_level)+'.pt')\n",
    "\n",
    "        # Balance datasets for graphs\n",
    "        X_train_pt, X_test_pt, y_train_pt, y_test_pt = balance_classes_pt(X_train_pt, X_test_pt, y_train_pt, y_test_pt)\n",
    "\n",
    "        # # Define graph\n",
    "        # graph = Data()\n",
    "        # graph.x = torch.cat((X_train_pt, X_test_pt))\n",
    "        # graph.y = torch.cat((y_train_pt, y_test_pt))\n",
    "        #\n",
    "        # # Train/test division\n",
    "        # graph.train_mask = torch.tensor([True]*len(X_train_pt)+[False]*len(X_test_pt))\n",
    "        # graph.test_mask = ~graph.train_mask\n",
    "        #\n",
    "        # # Edge creation\n",
    "        # input_df = pd.DataFrame(np.concatenate((X_train_unscaled[:,graph_cols].astype(int), X_test_unscaled[:,graph_cols].astype(int))))\n",
    "        # graph.edge_index = pd_to_adj_matrix(input_df, columns=list(range(len(graph_cols))), max_edges=max_edges)\n",
    "        #\n",
    "        # torch.save(graph, model_path+'graph-'+input_data+'-'+str(gap_filling_level)+'.pt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}